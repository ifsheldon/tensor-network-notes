{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:14.778649Z",
     "start_time": "2025-03-21T12:54:13.993968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#|default_exp tensor_decomposition\n",
    "#|export\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001fa9a9f13d0f7",
   "metadata": {},
   "source": [
    "# 1.8: 张量分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626dab245864461",
   "metadata": {},
   "source": [
    "## 单秩分解\n",
    "\n",
    "\n",
    "\n",
    "将 K 阶张量分解为 K 个向量的直积\n",
    "$$\n",
    "T = \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\n",
    "$$\n",
    "\n",
    "* 绝大多数张量不存在严格的单秩分解\n",
    "* $\\zeta$ 是常系数\n",
    "* 最优单秩近似问题：\n",
    "    * 向量限制为单位向量，保证数值稳定性；向量长度可以提取到 $\\zeta$ 里\n",
    "    $$\n",
    "    \\min_{\\zeta, \\{|\\boldsymbol{v}[k]|=1\\}} \\left|T - \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\\right|\n",
    "    $$\n",
    "\n",
    "\n",
    "Note:\n",
    "* 没有 entanglement 的多量子态就可以被单秩分解\n",
    "\n",
    "![tensor-rank-1-decomposition](images/tensor-rank-1-decomposition.png)\n",
    "\n",
    "\n",
    "### 最优单秩近似的迭代解法\n",
    "\n",
    "对于某个维度 $m$，收缩除了 $m$ 维度的所有维度：这些维度和对应的向量进行收缩，得到的结果就是 $m$ 维度对应的向量的近似\n",
    "\n",
    "$$\\sum_{\\{s_k,k\\neq m\\}} T_{s_0s_1...s_{K-1}} \\prod_{k\\neq m} v_{s_k}^{[k]*} = \\zeta v_{s_m}^{[m]}$$\n",
    "\n",
    "图例（以三阶张量为例）\n",
    "![tensor-decomposition-rank-1-example](images/rank-1-tensor-decomposition-iter-algorithm.png)\n",
    "\n",
    "伪代码：\n",
    "```\n",
    "初始化：\n",
    "* T 已知，它的阶数为 K\n",
    "* zeta 可以初始化为 1\n",
    "* 随机生成 K 个单位向量 v_0, v_1, ..., v_{K-1}\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    for i in range(K):\n",
    "        vs = [v[x] for x in range(K) and x != i]\n",
    "        scaled_vi = contract(T, vs)\n",
    "        vi = scaled_vi / norm(scaled_vi)\n",
    "        v[i] = vi\n",
    "\n",
    "    zeta_new = contract(T, v)\n",
    "    if |zeta_new - zeta| < eps: # Or the diff of vs is small\n",
    "        break\n",
    "    zeta = zeta_new\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad577ed735f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:14.784372Z",
     "start_time": "2025-03-21T12:54:14.781612Z"
    }
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def outer_product(vectors: List[torch.Tensor]) -> torch.Tensor:\n",
    "    for i, v in enumerate(vectors):\n",
    "        assert v.dim() == 1, f\"Expected 1D tensor, got {v.dim()}D tensor at index {i}\"\n",
    "    num_vectors = len(vectors)\n",
    "    assert num_vectors >= 2, \"At least two vectors are required for outer product\"\n",
    "    vec_dim_names = [f\"v{i}\" for i in range(num_vectors)]\n",
    "    einsum_exp = \"{input_string} -> {output_string}\".format(\n",
    "        input_string=\",\".join(vec_dim_names),\n",
    "        output_string=\" \".join(vec_dim_names),\n",
    "    )\n",
    "    return einops.einsum(*vectors, einsum_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be44accae1e4462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:14.891776Z",
     "start_time": "2025-03-21T12:54:14.887462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try\n",
    "T = torch.randn(2, 3, 4, dtype=torch.complex64)\n",
    "v0 = torch.randn(2, dtype=torch.complex64)\n",
    "v1 = torch.randn(3, dtype=torch.complex64)\n",
    "v2 = torch.randn(4, dtype=torch.complex64)\n",
    "\n",
    "# Try to calculate zeta differently\n",
    "zeta_ref = torch.einsum(\"abc, a, b, c ->\", T, v0, v1, v2)\n",
    "outer_v0v1v2 = outer_product([v0, v1, v2])\n",
    "zeta = (T * outer_v0v1v2).sum()\n",
    "assert zeta.isclose(zeta_ref)\n",
    "# Try to calculate v0 differently\n",
    "v0_ref = torch.einsum(\"abc,b,c -> a\", T, v1, v2)\n",
    "outer_v1v2 = outer_product([v1, v2]).unsqueeze(0)\n",
    "v0 = (T * outer_v1v2).sum((1, 2))\n",
    "assert v0.allclose(v0_ref)\n",
    "# Try to calculate v1 differently\n",
    "v1_ref = torch.einsum(\"abc,a,c -> b\", T, v0, v2, )\n",
    "outer_v0v2 = outer_product([v0, v2]).unsqueeze(1)\n",
    "v1 = (T * outer_v0v2).sum((0, 2))\n",
    "assert v1.allclose(v1_ref)\n",
    "# Try to calculate v2 differently\n",
    "v2_ref = torch.einsum(\"abc,a,b -> c\", T, v0, v1)\n",
    "outer_v0v1 = outer_product([v0, v1]).unsqueeze(2)\n",
    "v2 = (T * outer_v0v1).sum((0, 1))\n",
    "assert v2.allclose(v2_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876400a333fccda",
   "metadata": {},
   "source": [
    "#### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b684cf708594981d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:14.904150Z",
     "start_time": "2025-03-21T12:54:14.897840Z"
    }
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def rank1_tc(x, v=None, it_time=10000, tol=1e-14):\n",
    "    import torch as tc\n",
    "    # From: https://github.com/ranshiju/Python-for-Tensor-Network-Tutorial/blob/4c89b0766159d3495122ec39339e7bd019f10fdf/Library/MathFun.py#L231\n",
    "    # 初始化向量组v\n",
    "    if v is None:\n",
    "        v = list()\n",
    "        for n in range(x.ndimension()):\n",
    "            v.append(tc.randn(x.shape[n], device=x.device, dtype=x.dtype))\n",
    "\n",
    "    # 归一化向量组v\n",
    "    for n in range(x.ndimension()):\n",
    "        v[n] /= v[n].norm()\n",
    "\n",
    "    norm1 = 1\n",
    "    err = tc.ones(x.ndimension(), device=x.device, dtype=tc.float64)\n",
    "    err_norm = tc.ones(x.ndimension(), device=x.device, dtype=tc.float64)\n",
    "    for t in tqdm(range(it_time)):\n",
    "        for n in range(x.ndimension()):\n",
    "            x1 = x.clone()\n",
    "            for m in range(n):\n",
    "                # conj here because we need to \"cancel\" out vectors by taking the dot product\n",
    "                x1 = tc.tensordot(x1, v[m].conj(), [[0], [0]])\n",
    "            for m in range(len(v) - 1, n, -1):\n",
    "                x1 = tc.tensordot(x1, v[m].conj(), [[-1], [0]])\n",
    "            norm = x1.norm()\n",
    "            v1 = x1 / norm\n",
    "            err[n] = (v[n] - v1).norm()\n",
    "            err_norm[n] = (norm - norm1).norm()\n",
    "            v[n] = v1\n",
    "            norm1 = norm\n",
    "        if err.sum() / x.ndimension() < tol and err_norm.sum() / x.ndimension() < tol:\n",
    "            break\n",
    "    return v, norm1\n",
    "\n",
    "\n",
    "#|export\n",
    "def rank1_decomposition(tensor: torch.Tensor,\n",
    "                        num_iter: int = 10000,\n",
    "                        stop_criterion: str = \"zeta\",\n",
    "                        eps: float = 1e-14) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
    "    assert stop_criterion in [\"zeta\", \"norms\"]\n",
    "    device = tensor.device\n",
    "    t_shape = tensor.shape\n",
    "    k = len(t_shape)\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype, device=device) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    zeta = 1.\n",
    "\n",
    "    if stop_criterion == \"zeta\":\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for idx in range(k):\n",
    "                vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "                vs = [v.conj() for v in vs]\n",
    "                outer = outer_product(vs).unsqueeze(idx)\n",
    "                sum_indices = list(range(k))\n",
    "                sum_indices.pop(idx)\n",
    "                vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "                vi /= vi.norm()\n",
    "                decomposed_vecs[idx] = vi\n",
    "\n",
    "            # calculate zeta\n",
    "            vs = [v.conj() for v in decomposed_vecs]\n",
    "            zeta_new = (tensor * outer_product(vs)).sum().real\n",
    "            if (zeta_new - zeta).norm() < eps:\n",
    "                break\n",
    "            zeta = zeta_new\n",
    "    else:\n",
    "        # FIXME: seems to have bugs, adapted from the ref implementation, because the iteration takes a lot longer than my implementation - \"zeta\"\n",
    "        v_norm_diffs = torch.ones(k, dtype=torch.float32, device=device)\n",
    "        zeta_diffs = torch.ones(k, dtype=torch.float32, device=device)\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for idx in range(k):\n",
    "                # contraction\n",
    "                vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "                vs = [v.conj() for v in vs]\n",
    "                outer = outer_product(vs).unsqueeze(idx)\n",
    "                sum_indices = list(range(k))\n",
    "                sum_indices.pop(idx)\n",
    "                vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "                # calculate diffs\n",
    "                vi_norm = vi.norm()\n",
    "                vi /= vi_norm\n",
    "                v_norm_diffs[idx] = (decomposed_vecs[idx] - vi).norm()\n",
    "                zeta_diffs[idx] = (vi_norm - zeta).norm()\n",
    "                decomposed_vecs[idx] = vi\n",
    "                zeta = vi_norm\n",
    "\n",
    "            if v_norm_diffs.sum() / k < eps and zeta_diffs.sum() / k < eps:\n",
    "                break\n",
    "\n",
    "    return decomposed_vecs, zeta\n",
    "\n",
    "\n",
    "def rank1_decomposition_gradient_based(tensor: torch.Tensor, num_iter: int = 1000, eps: float = 1e-14) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
    "    t_shape = tensor.shape\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    for dv in decomposed_vecs:\n",
    "        dv.requires_grad_(True)\n",
    "\n",
    "    zeta = torch.ones(1, dtype=tensor.dtype)\n",
    "    adam = torch.optim.Adam(decomposed_vecs, lr=0.1)\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        outer = outer_product(decomposed_vecs)\n",
    "        loss = (tensor - outer).norm()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        adam.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            norms = [v.norm() for v in decomposed_vecs]\n",
    "            zeta_new = torch.prod(torch.tensor(norms))\n",
    "            if (zeta_new - zeta).norm() < eps:\n",
    "                break\n",
    "            else:\n",
    "                zeta = zeta_new\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "        return decomposed_vecs, zeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11458dff7b78db62",
   "metadata": {},
   "source": [
    "#### 测试及对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa23c616349e0e9",
   "metadata": {},
   "source": [
    "##### Iteration-based vs. Gradient-based Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973664f6de918afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:17.191650Z",
     "start_time": "2025-03-21T12:54:14.909847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c966035ce84744bf8fcb923618f8eb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bf0703cc314ba2b3efcaf45ce60fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparison between iteration-based optimization and gradient-based optimization\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions0, zeta0 = rank1_decomposition(a)\n",
    "decompositions1, zeta1 = rank1_decomposition_gradient_based(a)\n",
    "# Below assertions often fail because gradient-based optimization is less robust, since gradient-based optimization is hard to determine when to stop since the delta includes the gradient as well\n",
    "assert torch.allclose(zeta0, zeta1), f\"{zeta0}, {zeta1}\"\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f99e1bced01e2f",
   "metadata": {},
   "source": [
    "##### Test on Complex Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f024af43f8c60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:24.738022Z",
     "start_time": "2025-03-21T12:54:24.711309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf5f080461147f59eac276839723f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb55f606640444697d8df28c6938c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test for a complex tensor\n",
    "# This is sometimes flaky\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "decompositions0, zeta0 = rank1_decomposition(a)\n",
    "decompositions1, zeta1 = rank1_decomposition(a)\n",
    "assert torch.allclose(zeta0, zeta1), f\"{zeta0}, {zeta1}\"\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd11af158a7cac2",
   "metadata": {},
   "source": [
    "##### Test against Reference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4c86e3e6e7eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:29.094677Z",
     "start_time": "2025-03-21T12:54:27.833401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dd645e1aac4ff69d272c3971fb063d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2b2c20fd974840a1055c0fa2ce9a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "zeta=tensor(4.8214), zeta_ref=tensor(4.4708)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m decompositions, zeta = rank1_decomposition(a, stop_criterion=\u001b[33m\"\u001b[39m\u001b[33mzeta\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m decompositions_ref, zeta_ref = rank1_tc(a)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.isclose(zeta, zeta_ref), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzeta\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzeta_ref\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m outer = zeta * outer_product(decompositions)\n\u001b[32m      6\u001b[39m outer_ref = zeta_ref * outer_product(decompositions_ref)\n",
      "\u001b[31mAssertionError\u001b[39m: zeta=tensor(4.8214), zeta_ref=tensor(4.4708)"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "decompositions, zeta = rank1_decomposition(a, stop_criterion=\"zeta\")\n",
    "decompositions_ref, zeta_ref = rank1_tc(a)\n",
    "assert torch.isclose(zeta, zeta_ref), f\"{zeta=}, {zeta_ref=}\"\n",
    "outer = zeta * outer_product(decompositions)\n",
    "outer_ref = zeta_ref * outer_product(decompositions_ref)\n",
    "diff = (a - outer).norm()\n",
    "diff_ref = (a - outer_ref).norm()\n",
    "assert torch.isclose(diff, diff_ref), f\"{diff=}, {diff_ref=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bcf5e19516f34",
   "metadata": {},
   "source": [
    "##### Test Reference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3483ed1afe2c555b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:39.094631Z",
     "start_time": "2025-03-21T12:54:36.952785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab457c64a6e14f92bbf0ab89d435b6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e57b33d166c43d683244aff88ad090b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions0, zeta0 = rank1_tc(a)\n",
    "decompositions1, zeta1 = rank1_tc(a)\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da943c379b46a8",
   "metadata": {},
   "source": [
    "## Tucker Decomposition\n",
    "\n",
    "Tucker 分解 （i.e., Higher-Order Singular Value Decomposition, HOSVD）：$T_{i_0...i_{N-1}} = \\sum_{j_0...j_{N-1}}G_{j_0...j_{N-1}}\\prod_{n=0}^{N-1}U_{i_n j_n}^{(n)}$\n",
    "\n",
    "* **约束条件1**：$G$ 被称为核张量（core tensor），其各个指标对应的约化矩阵（reduced matrix）$\\overline{M}^{(n)}$ 须为非负实对角矩阵\n",
    "\n",
    "    $\\overline{M}^{(n)} \\coloneqq G_{[n]}G_{[n]}^{\\dagger} = \\begin{bmatrix} g_0 & 0 & \\cdots \\\\ 0 & g_1 & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}, g_0 \\geq g_1 \\geq \\cdots \\geq 0$\n",
    "    * $G_{[n]}$ 代表把 $G$ 的第 N 个指标作为左指标，其余作为右指标，作为矩阵\n",
    "\n",
    "* **约束条件2**：变换矩阵 $\\{U^{(n)}\\}(n=0,1...,N-1)$ 必须是 unitary\n",
    "\n",
    "* 通过奇异值分解获得变换矩阵：$T_{[n]} \\xrightarrow{\\text{SVD}} U^{(n)}S^{(n)}V^{(n)\\dagger}$\n",
    "\n",
    "* 核张量满足：$G_{i_0...i_{N-1}} = \\sum_{j_0...j_{N-1}}T_{j_0...j_{N-1}}\\prod_{n=0}^{N-1}U_{j_n i_n}^{(n)*}$\n",
    "\n",
    "* $T_{[n]}$的秩构成的Tucker秩\n",
    "\n",
    "以三阶张量为例，如下图\n",
    "\n",
    "![tucker-decomposition-example-order-3](images/tucker_decomposition_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154a2cd0c33878a",
   "metadata": {},
   "source": [
    "### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60911a053916207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:42.997835Z",
     "start_time": "2025-03-21T12:54:42.994768Z"
    }
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def make_matrix(tensor: torch.Tensor, left_index: int) -> torch.Tensor:\n",
    "    order = tensor.ndim\n",
    "    assert order >= 2\n",
    "    assert 0 <= left_index < order\n",
    "    t = torch.movedim(tensor, left_index, 0)\n",
    "    t = t.reshape(t.shape[0], -1)\n",
    "    return t\n",
    "\n",
    "#|export\n",
    "def tucker_decomposition(tensor: torch.Tensor) -> (torch.Tensor, List[torch.Tensor], List[int]):\n",
    "    order = tensor.ndim\n",
    "    assert order >= 2\n",
    "    matrices_U = []\n",
    "    ranks = []\n",
    "    for i in range(order):\n",
    "        matrix = make_matrix(tensor, i)\n",
    "        U, S, Vh = torch.linalg.svd(matrix)\n",
    "        matrices_U.append(U)\n",
    "        zeros = torch.zeros_like(S)\n",
    "        # see https://pytorch.org/docs/stable/generated/torch.linalg.matrix_rank.html\n",
    "        rtol = max(matrix.shape[0], matrix.shape[1]) * torch.finfo(matrix.dtype).eps\n",
    "        rank = (~torch.isclose(S, zeros, atol=1e-10, rtol=rtol)).to(torch.int32).sum().item()\n",
    "        ranks.append(rank)\n",
    "\n",
    "    core_tensor = tensor\n",
    "    for i in range(order):\n",
    "        core_tensor = torch.tensordot(core_tensor, matrices_U[i].conj(), dims=[[0], [0]])\n",
    "\n",
    "    return core_tensor, matrices_U, ranks\n",
    "\n",
    "#|export\n",
    "def reduced_matrix(core_tensor: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    order = core_tensor.ndim\n",
    "    assert order >= 2\n",
    "    assert 0 <= n < order\n",
    "    matrix = make_matrix(core_tensor, n)\n",
    "    return matrix @ matrix.conj().t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5295e5db7a60",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35adce65524ff2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:46.165201Z",
     "start_time": "2025-03-21T12:54:46.161469Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "core_tensor, matrices_U, ranks = tucker_decomposition(a)\n",
    "u0, u1, u2, u3 = matrices_U\n",
    "recovered = torch.einsum(\"abcd,ia,jb,kc,ld -> ijkl\", core_tensor, u0, u1, u2, u3)\n",
    "diff_norm = torch.dist(a, recovered)\n",
    "assert torch.isclose(diff_norm, torch.zeros_like(diff_norm), atol=1e-5), f\"{diff_norm=}\"\n",
    "\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "core_tensor, matrices_U, ranks = tucker_decomposition(a)\n",
    "u0, u1, u2, u3 = matrices_U\n",
    "recovered = torch.einsum(\"abcd,ia,jb,kc,ld -> ijkl\", core_tensor, u0, u1, u2, u3)\n",
    "diff_norm = torch.dist(a, recovered)\n",
    "assert torch.isclose(diff_norm, torch.zeros_like(diff_norm), atol=1e-5), f\"{diff_norm=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b42a97e0ce7a1f3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:54:48.628457Z",
     "start_time": "2025-03-21T12:54:48.625726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.9630e+01+0.0000e+00j, -9.5367e-07+2.9802e-07j,\n",
      "         -9.5367e-06+0.0000e+00j],\n",
      "        [-9.5367e-07-2.9802e-07j,  4.4821e+01+0.0000e+00j,\n",
      "         -7.2718e-06+7.1526e-07j],\n",
      "        [-1.1444e-05+7.1526e-07j, -8.2254e-06-9.5367e-07j,\n",
      "          4.0742e+01+0.0000e+00j]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "core_tensor, matrices_U, ranks = tucker_decomposition(a)\n",
    "rm1 = reduced_matrix(core_tensor, 1)\n",
    "print(rm1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
