{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T08:58:35.466913Z",
     "start_time": "2025-03-21T08:58:34.712531Z"
    }
   },
   "source": [
    "#|default_exp tensor_decomposition\n",
    "#|export\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.8: 张量分解",
   "id": "6001fa9a9f13d0f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 单秩分解\n",
    "\n",
    "\n",
    "\n",
    "将 K 阶张量分解为 K 个向量的直积\n",
    "$$\n",
    "T = \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\n",
    "$$\n",
    "\n",
    "* 绝大多数张量不存在严格的单秩分解\n",
    "* $\\zeta$ 是常系数\n",
    "* 最优单秩近似问题：\n",
    "    * 向量限制为单位向量，保证数值稳定性；向量长度可以提取到 $\\zeta$ 里\n",
    "    $$\n",
    "    \\min_{\\zeta, \\{|\\boldsymbol{v}[k]|=1\\}} \\left|T - \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\\right|\n",
    "    $$\n",
    "\n",
    "\n",
    "Note:\n",
    "* 没有 entanglement 的多量子态就可以被单秩分解\n",
    "\n",
    "![tensor-rank-1-decomposition](images/tensor-rank-1-decomposition.png)\n",
    "\n",
    "\n",
    "### 最优单秩近似的迭代解法\n",
    "\n",
    "对于某个维度 $m$，收缩除了 $m$ 维度的所有维度：这些维度和对应的向量进行收缩，得到的结果就是 $m$ 维度对应的向量的近似\n",
    "\n",
    "$$\\sum_{\\{s_k,k\\neq m\\}} T_{s_0s_1...s_{K-1}} \\prod_{k\\neq m} v_{s_k}^{[k]*} = \\zeta v_{s_m}^{[m]}$$\n",
    "\n",
    "图例（以三阶张量为例）\n",
    "![tensor-decomposition-rank-1-example](images/rank-1-tensor-decomposition-iter-algorithm.png)\n",
    "\n",
    "伪代码：\n",
    "```\n",
    "初始化：\n",
    "* T 已知，它的阶数为 K\n",
    "* zeta 可以初始化为 1\n",
    "* 随机生成 K 个单位向量 v_0, v_1, ..., v_{K-1}\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    for i in range(K):\n",
    "        vs = [v[x] for x in range(K) and x != i]\n",
    "        scaled_vi = contract(T, vs)\n",
    "        vi = scaled_vi / norm(scaled_vi)\n",
    "        v[i] = vi\n",
    "\n",
    "    zeta_new = contract(T, v)\n",
    "    if |zeta_new - zeta| < eps: # Or the diff of vs is small\n",
    "        break\n",
    "    zeta = zeta_new\n",
    "```\n",
    "\n"
   ],
   "id": "e626dab245864461"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:58:35.471736Z",
     "start_time": "2025-03-21T08:58:35.469529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def outer_product(vectors: List[torch.Tensor]) -> torch.Tensor:\n",
    "    for v in vectors:\n",
    "        assert v.dim() == 1\n",
    "    num_vectors = len(vectors)\n",
    "    assert num_vectors >= 2\n",
    "    # if num_vectors <= 26, we can just use einsum\n",
    "    if num_vectors <= 26:\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        input_string = \",\".join(c for c in alphabet[:num_vectors])\n",
    "        output_string = alphabet[:num_vectors]\n",
    "        einsum_exp = f\"{input_string} -> {output_string}\"\n",
    "        return torch.einsum(einsum_exp, *vectors)\n",
    "    else:\n",
    "        reshaped_vectors = []\n",
    "        for (i, v) in enumerate(vectors):\n",
    "            # calculate shapes\n",
    "            s = torch.ones(num_vectors, dtype=torch.int)\n",
    "            s[i] = v.shape[0]\n",
    "            s = s.tolist()\n",
    "            reshaped_vectors.append(v.reshape(s))\n",
    "\n",
    "        # broadcast multiplication\n",
    "        v = reshaped_vectors[0]\n",
    "        for i in range(1, num_vectors):\n",
    "            v = v * reshaped_vectors[i]\n",
    "        return v\n"
   ],
   "id": "97ad577ed735f1f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:58:35.569503Z",
     "start_time": "2025-03-21T08:58:35.565309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try\n",
    "T = torch.randn(2, 3, 4, dtype=torch.complex64)\n",
    "v0 = torch.randn(2, dtype=torch.complex64)\n",
    "v1 = torch.randn(3, dtype=torch.complex64)\n",
    "v2 = torch.randn(4, dtype=torch.complex64)\n",
    "\n",
    "# Try to calculate zeta differently\n",
    "zeta_ref = torch.einsum(\"abc, a, b, c ->\", T, v0, v1, v2)\n",
    "outer_v0v1v2 = outer_product([v0, v1, v2])\n",
    "zeta = (T * outer_v0v1v2).sum()\n",
    "assert zeta.isclose(zeta_ref)\n",
    "# Try to calculate v0 differently\n",
    "v0_ref = torch.einsum(\"abc,b,c -> a\", T, v1, v2)\n",
    "outer_v1v2 = outer_product([v1, v2]).unsqueeze(0)\n",
    "v0 = (T * outer_v1v2).sum((1, 2))\n",
    "assert v0.allclose(v0_ref)\n",
    "# Try to calculate v1 differently\n",
    "v1_ref = torch.einsum(\"abc,a,c -> b\", T, v0, v2, )\n",
    "outer_v0v2 = outer_product([v0, v2]).unsqueeze(1)\n",
    "v1 = (T * outer_v0v2).sum((0, 2))\n",
    "assert v1.allclose(v1_ref)\n",
    "# Try to calculate v2 differently\n",
    "v2_ref = torch.einsum(\"abc,a,b -> c\", T, v0, v1)\n",
    "outer_v0v1 = outer_product([v0, v1]).unsqueeze(2)\n",
    "v2 = (T * outer_v0v1).sum((0, 1))\n",
    "assert v2.allclose(v2_ref)"
   ],
   "id": "2be44accae1e4462",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 实现",
   "id": "e876400a333fccda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:13:16.741363Z",
     "start_time": "2025-03-21T10:13:16.734942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def rank1_tc(x, v=None, it_time=10000, tol=1e-14):\n",
    "    import torch as tc\n",
    "    # From: https://github.com/ranshiju/Python-for-Tensor-Network-Tutorial/blob/4c89b0766159d3495122ec39339e7bd019f10fdf/Library/MathFun.py#L231\n",
    "    # 初始化向量组v\n",
    "    if v is None:\n",
    "        v = list()\n",
    "        for n in range(x.ndimension()):\n",
    "            v.append(tc.randn(x.shape[n], device=x.device, dtype=x.dtype))\n",
    "\n",
    "    # 归一化向量组v\n",
    "    for n in range(x.ndimension()):\n",
    "        v[n] /= v[n].norm()\n",
    "\n",
    "    norm1 = 1\n",
    "    err = tc.ones(x.ndimension(), device=x.device, dtype=tc.float64)\n",
    "    err_norm = tc.ones(x.ndimension(), device=x.device, dtype=tc.float64)\n",
    "    for t in tqdm(range(it_time)):\n",
    "        for n in range(x.ndimension()):\n",
    "            x1 = x.clone()\n",
    "            for m in range(n):\n",
    "                # TODO: why do we need conj here? same for my implementation of rank1_decomposition\n",
    "                x1 = tc.tensordot(x1, v[m].conj(), [[0], [0]])\n",
    "            for m in range(len(v) - 1, n, -1):\n",
    "                x1 = tc.tensordot(x1, v[m].conj(), [[-1], [0]])\n",
    "            norm = x1.norm()\n",
    "            v1 = x1 / norm\n",
    "            err[n] = (v[n] - v1).norm()\n",
    "            err_norm[n] = (norm - norm1).norm()\n",
    "            v[n] = v1\n",
    "            norm1 = norm\n",
    "        if err.sum() / x.ndimension() < tol and err_norm.sum() / x.ndimension() < tol:\n",
    "            break\n",
    "    return v, norm1\n",
    "\n",
    "\n",
    "#|export\n",
    "def rank1_decomposition(tensor: torch.Tensor,\n",
    "                        num_iter: int = 10000,\n",
    "                        stop_criterion: str = \"zeta\",\n",
    "                        eps: float = 1e-14) -> (List[torch.Tensor], torch.Tensor):\n",
    "    assert stop_criterion in [\"zeta\", \"norms\"]\n",
    "    device = tensor.device\n",
    "    t_shape = tensor.shape\n",
    "    k = len(t_shape)\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype, device=device) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    zeta = 1.\n",
    "\n",
    "    if stop_criterion == \"zeta\":\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for idx in range(k):\n",
    "                vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "                vs = [v.conj() for v in vs]\n",
    "                outer = outer_product(vs).unsqueeze(idx)\n",
    "                sum_indices = list(range(k))\n",
    "                sum_indices.pop(idx)\n",
    "                vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "                vi /= vi.norm()\n",
    "                decomposed_vecs[idx] = vi\n",
    "\n",
    "            # calculate zeta\n",
    "            vs = [v.conj() for v in decomposed_vecs]\n",
    "            zeta_new = (tensor * outer_product(vs)).sum().real\n",
    "            if (zeta_new - zeta).norm() < eps:\n",
    "                break\n",
    "            zeta = zeta_new\n",
    "    else:\n",
    "        # FIXME: seems to have bugs, adapted from the ref implementation, because the iteration takes a lot longer than my implementation - \"zeta\"\n",
    "        v_norm_diffs = torch.ones(k, dtype=torch.float32, device=device)\n",
    "        zeta_diffs = torch.ones(k, dtype=torch.float32, device=device)\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for idx in range(k):\n",
    "                # contraction\n",
    "                vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "                vs = [v.conj() for v in vs]\n",
    "                outer = outer_product(vs).unsqueeze(idx)\n",
    "                sum_indices = list(range(k))\n",
    "                sum_indices.pop(idx)\n",
    "                vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "                # calculate diffs\n",
    "                vi_norm = vi.norm()\n",
    "                vi /= vi_norm\n",
    "                v_norm_diffs[idx] = (decomposed_vecs[idx] - vi).norm()\n",
    "                zeta_diffs[idx] = (vi_norm - zeta).norm()\n",
    "                decomposed_vecs[idx] = vi\n",
    "                zeta = vi_norm\n",
    "\n",
    "            if v_norm_diffs.sum() / k < eps and zeta_diffs.sum() / k < eps:\n",
    "                break\n",
    "\n",
    "    return decomposed_vecs, zeta\n",
    "\n",
    "\n",
    "def rank1_decomposition_gradient_based(tensor: torch.Tensor, num_iter: int = 1000, eps: float = 1e-14) -> (\n",
    "        List[torch.Tensor], torch.Tensor):\n",
    "    t_shape = tensor.shape\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    for dv in decomposed_vecs:\n",
    "        dv.requires_grad_(True)\n",
    "\n",
    "    zeta = torch.ones(1, dtype=tensor.dtype)\n",
    "    adam = torch.optim.Adam(decomposed_vecs, lr=0.1)\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        outer = outer_product(decomposed_vecs)\n",
    "        loss = (tensor - outer).norm()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        adam.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            norms = [v.norm() for v in decomposed_vecs]\n",
    "            zeta_new = torch.prod(torch.tensor(norms))\n",
    "            if (zeta_new - zeta).norm() < eps:\n",
    "                break\n",
    "            else:\n",
    "                zeta = zeta_new\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "        return decomposed_vecs, zeta"
   ],
   "id": "b684cf708594981d",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 测试及对比",
   "id": "11458dff7b78db62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Iteration-based vs. Gradient-based Optimization",
   "id": "3aa23c616349e0e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:26:58.248651Z",
     "start_time": "2025-03-21T10:26:58.125135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparison between iteration-based optimization and gradient-based optimization\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions0, zeta0 = rank1_decomposition(a)\n",
    "decompositions1, zeta1 = rank1_decomposition_gradient_based(a)\n",
    "# Below assertions often fail because gradient-based optimization is less robust, since gradient-based optimization is hard to determine when to stop since the delta includes the gradient as well\n",
    "assert torch.allclose(zeta0, zeta1), f\"{zeta0}, {zeta1}\"\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ],
   "id": "973664f6de918afa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2c52a1b734a45238260fd3e402a0aa5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45154e547c304885ad92573c7e17c7ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Test on Complex Tensor",
   "id": "30f99e1bced01e2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:27:08.723141Z",
     "start_time": "2025-03-21T10:27:08.698112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test for a complex tensor\n",
    "# This is sometimes flaky\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "decompositions0, zeta0 = rank1_decomposition(a)\n",
    "decompositions1, zeta1 = rank1_decomposition(a)\n",
    "assert torch.allclose(zeta0, zeta1), f\"{zeta0}, {zeta1}\"\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ],
   "id": "72f024af43f8c60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf846bad16114aa2afbdbbaa6d989f8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cbc7db2aa62439bb2d3cee63c08c499"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Test against Reference Implementation",
   "id": "7bd11af158a7cac2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:18.231628Z",
     "start_time": "2025-03-21T10:25:16.992671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "decompositions, zeta = rank1_decomposition(a, stop_criterion=\"zeta\")\n",
    "decompositions_ref, zeta_ref = rank1_tc(a)\n",
    "assert torch.isclose(zeta, zeta_ref), f\"{zeta=}, {zeta_ref=}\"\n",
    "outer = zeta * outer_product(decompositions)\n",
    "outer_ref = zeta_ref * outer_product(decompositions_ref)\n",
    "diff = (a - outer).norm()\n",
    "diff_ref = (a - outer_ref).norm()\n",
    "assert torch.isclose(diff, diff_ref), f\"{diff=}, {diff_ref=}\""
   ],
   "id": "dd4c86e3e6e7eb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3b5384b006743598aca0a673aa9b74e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af6a14afca90475c8366458b780cc0c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Test Reference Implementation",
   "id": "762bcf5e19516f34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:44.046995Z",
     "start_time": "2025-03-21T10:25:41.905371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions0, zeta0 = rank1_tc(a)\n",
    "decompositions1, zeta1 = rank1_tc(a)\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ],
   "id": "3483ed1afe2c555b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e561945f12ec4219bce37a1b6f343336"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0a46f453c78447da116b9d7b561677d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 98
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
