{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T12:46:47.344876Z",
     "start_time": "2025-03-21T12:46:46.568568Z"
    }
   },
   "source": [
    "#|default_exp tensor_decomposition\n",
    "#|export\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.8: 张量分解",
   "id": "6001fa9a9f13d0f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 单秩分解\n",
    "\n",
    "\n",
    "\n",
    "将 K 阶张量分解为 K 个向量的直积\n",
    "$$\n",
    "T = \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\n",
    "$$\n",
    "\n",
    "* 绝大多数张量不存在严格的单秩分解\n",
    "* $\\zeta$ 是常系数\n",
    "* 最优单秩近似问题：\n",
    "    * 向量限制为单位向量，保证数值稳定性；向量长度可以提取到 $\\zeta$ 里\n",
    "    $$\n",
    "    \\min_{\\zeta, \\{|\\boldsymbol{v}[k]|=1\\}} \\left|T - \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\\right|\n",
    "    $$\n",
    "\n",
    "\n",
    "Note:\n",
    "* 没有 entanglement 的多量子态就可以被单秩分解\n",
    "\n",
    "![tensor-rank-1-decomposition](images/tensor-rank-1-decomposition.png)\n",
    "\n",
    "\n",
    "### 最优单秩近似的迭代解法\n",
    "\n",
    "对于某个维度 $m$，收缩除了 $m$ 维度的所有维度：这些维度和对应的向量进行收缩，得到的结果就是 $m$ 维度对应的向量的近似\n",
    "\n",
    "$$\\sum_{\\{s_k,k\\neq m\\}} T_{s_0s_1...s_{K-1}} \\prod_{k\\neq m} v_{s_k}^{[k]*} = \\zeta v_{s_m}^{[m]}$$\n",
    "\n",
    "图例（以三阶张量为例）\n",
    "![tensor-decomposition-rank-1-example](images/rank-1-tensor-decomposition-iter-algorithm.png)\n",
    "\n",
    "伪代码：\n",
    "```\n",
    "初始化：\n",
    "* T 已知，它的阶数为 K\n",
    "* zeta 可以初始化为 1\n",
    "* 随机生成 K 个单位向量 v_0, v_1, ..., v_{K-1}\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    for i in range(K):\n",
    "        vs = [v[x] for x in range(K) and x != i]\n",
    "        scaled_vi = contract(T, vs)\n",
    "        vi = scaled_vi / norm(scaled_vi)\n",
    "        v[i] = vi\n",
    "\n",
    "    zeta_new = contract(T, v)\n",
    "    if |zeta_new - zeta| < eps: # Or the diff of vs is small\n",
    "        break\n",
    "    zeta = zeta_new\n",
    "```\n",
    "\n"
   ],
   "id": "e626dab245864461"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:46:47.350571Z",
     "start_time": "2025-03-21T12:46:47.347891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def outer_product(vectors: List[torch.Tensor]) -> torch.Tensor:\n",
    "    for v in vectors:\n",
    "        assert v.dim() == 1\n",
    "    num_vectors = len(vectors)\n",
    "    assert num_vectors >= 2\n",
    "    # if num_vectors <= 26, we can just use einsum\n",
    "    if num_vectors <= 26:\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        input_string = \",\".join(c for c in alphabet[:num_vectors])\n",
    "        output_string = alphabet[:num_vectors]\n",
    "        einsum_exp = f\"{input_string} -> {output_string}\"\n",
    "        return torch.einsum(einsum_exp, *vectors)\n",
    "    else:\n",
    "        reshaped_vectors = []\n",
    "        for (i, v) in enumerate(vectors):\n",
    "            # calculate shapes\n",
    "            s = torch.ones(num_vectors, dtype=torch.int)\n",
    "            s[i] = v.shape[0]\n",
    "            s = s.tolist()\n",
    "            reshaped_vectors.append(v.reshape(s))\n",
    "\n",
    "        # broadcast multiplication\n",
    "        v = reshaped_vectors[0]\n",
    "        for i in range(1, num_vectors):\n",
    "            v = v * reshaped_vectors[i]\n",
    "        return v\n"
   ],
   "id": "97ad577ed735f1f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:46:47.450071Z",
     "start_time": "2025-03-21T12:46:47.445745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try\n",
    "T = torch.randn(2, 3, 4, dtype=torch.complex64)\n",
    "v0 = torch.randn(2, dtype=torch.complex64)\n",
    "v1 = torch.randn(3, dtype=torch.complex64)\n",
    "v2 = torch.randn(4, dtype=torch.complex64)\n",
    "\n",
    "# Try to calculate zeta differently\n",
    "zeta_ref = torch.einsum(\"abc, a, b, c ->\", T, v0, v1, v2)\n",
    "outer_v0v1v2 = outer_product([v0, v1, v2])\n",
    "zeta = (T * outer_v0v1v2).sum()\n",
    "assert zeta.isclose(zeta_ref)\n",
    "# Try to calculate v0 differently\n",
    "v0_ref = torch.einsum(\"abc,b,c -> a\", T, v1, v2)\n",
    "outer_v1v2 = outer_product([v1, v2]).unsqueeze(0)\n",
    "v0 = (T * outer_v1v2).sum((1, 2))\n",
    "assert v0.allclose(v0_ref)\n",
    "# Try to calculate v1 differently\n",
    "v1_ref = torch.einsum(\"abc,a,c -> b\", T, v0, v2, )\n",
    "outer_v0v2 = outer_product([v0, v2]).unsqueeze(1)\n",
    "v1 = (T * outer_v0v2).sum((0, 2))\n",
    "assert v1.allclose(v1_ref)\n",
    "# Try to calculate v2 differently\n",
    "v2_ref = torch.einsum(\"abc,a,b -> c\", T, v0, v1)\n",
    "outer_v0v1 = outer_product([v0, v1]).unsqueeze(2)\n",
    "v2 = (T * outer_v0v1).sum((0, 1))\n",
    "assert v2.allclose(v2_ref)"
   ],
   "id": "2be44accae1e4462",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 实现",
   "id": "e876400a333fccda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:46:47.463073Z",
     "start_time": "2025-03-21T12:46:47.456492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def rank1_tc(x, v=None, it_time=10000, tol=1e-14):\n",
    "    import torch as tc\n",
    "    # From: https://github.com/ranshiju/Python-for-Tensor-Network-Tutorial/blob/4c89b0766159d3495122ec39339e7bd019f10fdf/Library/MathFun.py#L231\n",
    "    # 初始化向量组v\n",
    "    if v is None:\n",
    "        v = list()\n",
    "        for n in range(x.ndimension()):\n",
    "            v.append(tc.randn(x.shape[n], device=x.device, dtype=x.dtype))\n",
    "\n",
    "    # 归一化向量组v\n",
    "    for n in range(x.ndimension()):\n",
    "        v[n] /= v[n].norm()\n",
    "\n",
    "    norm1 = 1\n",
    "    err = tc.ones(x.ndimension(), device=x.device, dtype=tc.float64)\n",
    "    err_norm = tc.ones(x.ndimension(), device=x.device, dtype=tc.float64)\n",
    "    for t in tqdm(range(it_time)):\n",
    "        for n in range(x.ndimension()):\n",
    "            x1 = x.clone()\n",
    "            for m in range(n):\n",
    "                # TODO: why do we need conj here? same for my implementation of rank1_decomposition\n",
    "                x1 = tc.tensordot(x1, v[m].conj(), [[0], [0]])\n",
    "            for m in range(len(v) - 1, n, -1):\n",
    "                x1 = tc.tensordot(x1, v[m].conj(), [[-1], [0]])\n",
    "            norm = x1.norm()\n",
    "            v1 = x1 / norm\n",
    "            err[n] = (v[n] - v1).norm()\n",
    "            err_norm[n] = (norm - norm1).norm()\n",
    "            v[n] = v1\n",
    "            norm1 = norm\n",
    "        if err.sum() / x.ndimension() < tol and err_norm.sum() / x.ndimension() < tol:\n",
    "            break\n",
    "    return v, norm1\n",
    "\n",
    "\n",
    "#|export\n",
    "def rank1_decomposition(tensor: torch.Tensor,\n",
    "                        num_iter: int = 10000,\n",
    "                        stop_criterion: str = \"zeta\",\n",
    "                        eps: float = 1e-14) -> (List[torch.Tensor], torch.Tensor):\n",
    "    assert stop_criterion in [\"zeta\", \"norms\"]\n",
    "    device = tensor.device\n",
    "    t_shape = tensor.shape\n",
    "    k = len(t_shape)\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype, device=device) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    zeta = 1.\n",
    "\n",
    "    if stop_criterion == \"zeta\":\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for idx in range(k):\n",
    "                vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "                vs = [v.conj() for v in vs]\n",
    "                outer = outer_product(vs).unsqueeze(idx)\n",
    "                sum_indices = list(range(k))\n",
    "                sum_indices.pop(idx)\n",
    "                vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "                vi /= vi.norm()\n",
    "                decomposed_vecs[idx] = vi\n",
    "\n",
    "            # calculate zeta\n",
    "            vs = [v.conj() for v in decomposed_vecs]\n",
    "            zeta_new = (tensor * outer_product(vs)).sum().real\n",
    "            if (zeta_new - zeta).norm() < eps:\n",
    "                break\n",
    "            zeta = zeta_new\n",
    "    else:\n",
    "        # FIXME: seems to have bugs, adapted from the ref implementation, because the iteration takes a lot longer than my implementation - \"zeta\"\n",
    "        v_norm_diffs = torch.ones(k, dtype=torch.float32, device=device)\n",
    "        zeta_diffs = torch.ones(k, dtype=torch.float32, device=device)\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            for idx in range(k):\n",
    "                # contraction\n",
    "                vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "                vs = [v.conj() for v in vs]\n",
    "                outer = outer_product(vs).unsqueeze(idx)\n",
    "                sum_indices = list(range(k))\n",
    "                sum_indices.pop(idx)\n",
    "                vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "                # calculate diffs\n",
    "                vi_norm = vi.norm()\n",
    "                vi /= vi_norm\n",
    "                v_norm_diffs[idx] = (decomposed_vecs[idx] - vi).norm()\n",
    "                zeta_diffs[idx] = (vi_norm - zeta).norm()\n",
    "                decomposed_vecs[idx] = vi\n",
    "                zeta = vi_norm\n",
    "\n",
    "            if v_norm_diffs.sum() / k < eps and zeta_diffs.sum() / k < eps:\n",
    "                break\n",
    "\n",
    "    return decomposed_vecs, zeta\n",
    "\n",
    "\n",
    "def rank1_decomposition_gradient_based(tensor: torch.Tensor, num_iter: int = 1000, eps: float = 1e-14) -> (\n",
    "        List[torch.Tensor], torch.Tensor):\n",
    "    t_shape = tensor.shape\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    for dv in decomposed_vecs:\n",
    "        dv.requires_grad_(True)\n",
    "\n",
    "    zeta = torch.ones(1, dtype=tensor.dtype)\n",
    "    adam = torch.optim.Adam(decomposed_vecs, lr=0.1)\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        outer = outer_product(decomposed_vecs)\n",
    "        loss = (tensor - outer).norm()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        adam.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            norms = [v.norm() for v in decomposed_vecs]\n",
    "            zeta_new = torch.prod(torch.tensor(norms))\n",
    "            if (zeta_new - zeta).norm() < eps:\n",
    "                break\n",
    "            else:\n",
    "                zeta = zeta_new\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "        return decomposed_vecs, zeta"
   ],
   "id": "b684cf708594981d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 测试及对比",
   "id": "11458dff7b78db62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Iteration-based vs. Gradient-based Optimization",
   "id": "3aa23c616349e0e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:47:02.556367Z",
     "start_time": "2025-03-21T12:47:02.496372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparison between iteration-based optimization and gradient-based optimization\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions0, zeta0 = rank1_decomposition(a)\n",
    "decompositions1, zeta1 = rank1_decomposition_gradient_based(a)\n",
    "# Below assertions often fail because gradient-based optimization is less robust, since gradient-based optimization is hard to determine when to stop since the delta includes the gradient as well\n",
    "assert torch.allclose(zeta0, zeta1), f\"{zeta0}, {zeta1}\"\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ],
   "id": "973664f6de918afa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d45e13a16d674311b439d432a0e301ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bb89e760ff64d3babcdf3fed2f2179f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Test on Complex Tensor",
   "id": "30f99e1bced01e2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:47:07.146663Z",
     "start_time": "2025-03-21T12:47:07.130303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test for a complex tensor\n",
    "# This is sometimes flaky\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "decompositions0, zeta0 = rank1_decomposition(a)\n",
    "decompositions1, zeta1 = rank1_decomposition(a)\n",
    "assert torch.allclose(zeta0, zeta1), f\"{zeta0}, {zeta1}\"\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ],
   "id": "72f024af43f8c60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51683b3ec279495fb69d97e3566260cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fa21d500bd84640b2b504bda5222b34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Test against Reference Implementation",
   "id": "7bd11af158a7cac2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:47:33.275968Z",
     "start_time": "2025-03-21T12:47:31.966681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "decompositions, zeta = rank1_decomposition(a, stop_criterion=\"zeta\")\n",
    "decompositions_ref, zeta_ref = rank1_tc(a)\n",
    "assert torch.isclose(zeta, zeta_ref), f\"{zeta=}, {zeta_ref=}\"\n",
    "outer = zeta * outer_product(decompositions)\n",
    "outer_ref = zeta_ref * outer_product(decompositions_ref)\n",
    "diff = (a - outer).norm()\n",
    "diff_ref = (a - outer_ref).norm()\n",
    "assert torch.isclose(diff, diff_ref), f\"{diff=}, {diff_ref=}\""
   ],
   "id": "dd4c86e3e6e7eb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "992ea0ac53604397a27d2b2ecdac11ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c297cf8fb0a44aab84daf4ce521f2ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Test Reference Implementation",
   "id": "762bcf5e19516f34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:47:44.185177Z",
     "start_time": "2025-03-21T12:47:42.008753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions0, zeta0 = rank1_tc(a)\n",
    "decompositions1, zeta1 = rank1_tc(a)\n",
    "outer0 = zeta0 * outer_product(decompositions0)\n",
    "outer1 = zeta1 * outer_product(decompositions1)\n",
    "diff0 = (a - outer0).norm()\n",
    "diff1 = (a - outer1).norm()\n",
    "assert torch.isclose(diff0, diff1), f\"{diff0=}, {diff1=}\""
   ],
   "id": "3483ed1afe2c555b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55e227fb41d64067a07a0a0fdf434131"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "824e8ee9aa294351bcf8b4d2f6b6fb3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tucker Decomposition\n",
    "\n",
    "Tucker 分解 （i.e., Higher-Order Singular Value Decomposition, HOSVD）：$T_{i_0...i_{N-1}} = \\sum_{j_0...j_{N-1}}G_{j_0...j_{N-1}}\\prod_{n=0}^{N-1}U_{i_n j_n}^{(n)}$\n",
    "\n",
    "* **约束条件1**：$G$ 被称为核张量（core tensor），其各个指标对应的约化矩阵（reduced matrix）$\\overline{M}^{(n)}$ 须为非负实对角矩阵\n",
    "\n",
    "    $\\overline{M}^{(n)} \\coloneqq G_{[n]}G_{[n]}^{\\dagger} = \\begin{bmatrix} g_0 & 0 & \\cdots \\\\ 0 & g_1 & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}, g_0 \\geq g_1 \\geq \\cdots \\geq 0$\n",
    "    * $G_{[n]}$ 代表把 $G$ 的第 N 个指标作为左指标，其余作为右指标，作为矩阵\n",
    "\n",
    "* **约束条件2**：变换矩阵 $\\{U^{(n)}\\}(n=0,1...,N-1)$ 必须是 unitary\n",
    "\n",
    "* 通过奇异值分解获得变换矩阵：$T_{[n]} \\xrightarrow{\\text{SVD}} U^{(n)}S^{(n)}V^{(n)\\dagger}$\n",
    "\n",
    "* 核张量满足：$G_{i_0...i_{N-1}} = \\sum_{j_0...j_{N-1}}T_{j_0...j_{N-1}}\\prod_{n=0}^{N-1}U_{j_n i_n}^{(n)*}$\n",
    "\n",
    "* $T_{[n]}$的秩构成的Tucker秩\n",
    "\n",
    "以三阶张量为例，如下图\n",
    "\n",
    "![tucker-decomposition-example-order-3](images/tucker_decomposition_example.png)"
   ],
   "id": "b6da943c379b46a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 实现",
   "id": "8154a2cd0c33878a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:47:47.655081Z",
     "start_time": "2025-03-21T12:47:47.651966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def make_matrix(tensor: torch.Tensor, left_index: int) -> torch.Tensor:\n",
    "    order = tensor.ndim\n",
    "    assert order >= 2\n",
    "    assert 0 <= left_index < order\n",
    "    t = torch.movedim(tensor, left_index, 0)\n",
    "    t = t.reshape(t.shape[0], -1)\n",
    "    return t\n",
    "\n",
    "#|export\n",
    "def tucker_decomposition(tensor: torch.Tensor) -> (torch.Tensor, List[torch.Tensor], List[int]):\n",
    "    order = tensor.ndim\n",
    "    assert order >= 2\n",
    "    matrices_U = []\n",
    "    ranks = []\n",
    "    for i in range(order):\n",
    "        matrix = make_matrix(tensor, i)\n",
    "        U, S, Vh = torch.linalg.svd(matrix)\n",
    "        matrices_U.append(U)\n",
    "        zeros = torch.zeros_like(S)\n",
    "        # see https://pytorch.org/docs/stable/generated/torch.linalg.matrix_rank.html\n",
    "        rtol = max(matrix.shape[0], matrix.shape[1]) * torch.finfo(matrix.dtype).eps\n",
    "        rank = (~torch.isclose(S, zeros, atol=1e-10, rtol=rtol)).to(torch.int32).sum().item()\n",
    "        ranks.append(rank)\n",
    "\n",
    "    core_tensor = tensor\n",
    "    for i in range(order):\n",
    "        core_tensor = torch.tensordot(core_tensor, matrices_U[i].conj(), dims=[[0], [0]])\n",
    "\n",
    "    return core_tensor, matrices_U, ranks"
   ],
   "id": "e60911a053916207",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 测试",
   "id": "2df5295e5db7a60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:47:50.572650Z",
     "start_time": "2025-03-21T12:47:50.569088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.complex64)\n",
    "core_tensor, matrices_U, ranks = tucker_decomposition(a)\n",
    "u0, u1, u2, u3 = matrices_U\n",
    "recovered = torch.einsum(\"abcd,ia,jb,kc,ld -> ijkl\", core_tensor, u0, u1, u2, u3)\n",
    "diff_norm = torch.dist(a, recovered)\n",
    "assert torch.isclose(diff_norm, torch.zeros_like(diff_norm), atol=1e-5), f\"{diff_norm=}\"\n",
    "\n",
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "core_tensor, matrices_U, ranks = tucker_decomposition(a)\n",
    "u0, u1, u2, u3 = matrices_U\n",
    "recovered = torch.einsum(\"abcd,ia,jb,kc,ld -> ijkl\", core_tensor, u0, u1, u2, u3)\n",
    "diff_norm = torch.dist(a, recovered)\n",
    "assert torch.isclose(diff_norm, torch.zeros_like(diff_norm), atol=1e-5), f\"{diff_norm=}\""
   ],
   "id": "35adce65524ff2bc",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
