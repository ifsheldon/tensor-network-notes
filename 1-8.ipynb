{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T15:35:54.596295Z",
     "start_time": "2025-03-20T15:35:53.609694Z"
    }
   },
   "source": [
    "#|default_exp tensor_decomposition\n",
    "#|export\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.8: 张量分解",
   "id": "6001fa9a9f13d0f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 单秩分解\n",
    "\n",
    "\n",
    "\n",
    "将 K 阶张量分解为 K 个向量的直积\n",
    "$$\n",
    "T = \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\n",
    "$$\n",
    "\n",
    "* 绝大多数张量不存在严格的单秩分解\n",
    "* $\\zeta$ 是常系数\n",
    "* 最优单秩近似问题：\n",
    "    * 向量限制为单位向量，保证数值稳定性；向量长度可以提取到 $\\zeta$ 里\n",
    "    $$\n",
    "    \\min_{\\zeta, \\{|\\boldsymbol{v}[k]|=1\\}} \\left|T - \\zeta \\prod_{\\otimes k=0}^{K-1} \\boldsymbol{v}^{(k)}\\right|\n",
    "    $$\n",
    "\n",
    "\n",
    "Note:\n",
    "* 没有 entanglement 的多量子态就可以被单秩分解\n",
    "\n",
    "![tensor-rank-1-decomposition](images/tensor-rank-1-decomposition.png)\n",
    "\n",
    "TODO:\n",
    "* 看书 1.6\n",
    "* Rank 1 分解的代码\n",
    "\n",
    "\n",
    "\n",
    "## 最优单秩近似的迭代解法\n",
    "\n",
    "对于某个维度 $m$，收缩除了 $m$ 维度的所有维度：这些维度和对应的向量进行收缩，得到的结果就是 $m$ 维度对应的向量的近似\n",
    "\n",
    "$$\\sum_{\\{s_k,k\\neq m\\}} T_{s_0s_1...s_{K-1}} \\prod_{k\\neq m} v_{s_k}^{[k]*} = \\zeta v_{s_m}^{[m]}$$\n",
    "\n",
    "图例（以三阶张量为例）\n",
    "![tensor-decomposition-rank-1-example](images/rank-1-tensor-decomposition-iter-algorithm.png)\n",
    "\n",
    "伪代码：\n",
    "```\n",
    "初始化：\n",
    "* T 已知，它的阶数为 K\n",
    "* zeta 可以初始化为 1\n",
    "* 随机生成 K 个单位向量 v_0, v_1, ..., v_{K-1}\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    for i in range(K):\n",
    "        vs = [v[x] for x in range(K) and x != i]\n",
    "        scaled_vi = contract(T, vs)\n",
    "        vi = scaled_vi / norm(scaled_vi)\n",
    "        v[i] = vi\n",
    "\n",
    "    zeta_new = contract(T, v)\n",
    "    if |zeta_new - zeta| < eps: # Or the diff of vs is small\n",
    "        break\n",
    "    zeta = zeta_new\n",
    "```\n",
    "\n"
   ],
   "id": "e626dab245864461"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T15:35:54.601708Z",
     "start_time": "2025-03-20T15:35:54.599004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def outer_product(vectors: List[torch.Tensor]) -> torch.Tensor:\n",
    "    for v in vectors:\n",
    "        assert v.dim() == 1\n",
    "    num_vectors = len(vectors)\n",
    "    assert num_vectors >= 2\n",
    "    # if num_vectors <= 26, we can just use einsum\n",
    "    if num_vectors <= 26:\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        input_string = \",\".join(c for c in alphabet[:num_vectors])\n",
    "        output_string = alphabet[:num_vectors]\n",
    "        einsum_exp = f\"{input_string} -> {output_string}\"\n",
    "        return torch.einsum(einsum_exp, *vectors)\n",
    "    else:\n",
    "        reshaped_vectors = []\n",
    "        for (i, v) in enumerate(vectors):\n",
    "            # calculate shapes\n",
    "            s = torch.ones(num_vectors, dtype=torch.int)\n",
    "            s[i] = v.shape[0]\n",
    "            s = s.tolist()\n",
    "            reshaped_vectors.append(v.reshape(s))\n",
    "\n",
    "        # broadcast multiplication\n",
    "        v = reshaped_vectors[0]\n",
    "        for i in range(1, num_vectors):\n",
    "            v = v * reshaped_vectors[i]\n",
    "        return v\n"
   ],
   "id": "97ad577ed735f1f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T15:35:54.705696Z",
     "start_time": "2025-03-20T15:35:54.701604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try\n",
    "T = torch.randn(2, 3, 4, dtype=torch.complex64)\n",
    "v0 = torch.randn(2, dtype=torch.complex64)\n",
    "v1 = torch.randn(3, dtype=torch.complex64)\n",
    "v2 = torch.randn(4, dtype=torch.complex64)\n",
    "\n",
    "# Try to calculate zeta differently\n",
    "zeta_ref = torch.einsum(\"abc, a, b, c ->\", T, v0, v1, v2)\n",
    "outer_v0v1v2 = outer_product([v0, v1, v2])\n",
    "zeta = (T * outer_v0v1v2).sum()\n",
    "assert zeta.isclose(zeta_ref)\n",
    "# Try to calculate v0 differently\n",
    "v0_ref = torch.einsum(\"abc,b,c -> a\", T, v1, v2)\n",
    "outer_v1v2 = outer_product([v1, v2]).unsqueeze(0)\n",
    "v0 = (T * outer_v1v2).sum((1, 2))\n",
    "assert v0.allclose(v0_ref)\n",
    "# Try to calculate v1 differently\n",
    "v1_ref = torch.einsum(\"abc,a,c -> b\", T, v0, v2, )\n",
    "outer_v0v2 = outer_product([v0, v2]).unsqueeze(1)\n",
    "v1 = (T * outer_v0v2).sum((0, 2))\n",
    "assert v1.allclose(v1_ref)\n",
    "# Try to calculate v2 differently\n",
    "v2_ref = torch.einsum(\"abc,a,b -> c\", T, v0, v1)\n",
    "outer_v0v1 = outer_product([v0, v1]).unsqueeze(2)\n",
    "v2 = (T * outer_v0v1).sum((0, 1))\n",
    "assert v2.allclose(v2_ref)"
   ],
   "id": "2be44accae1e4462",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T15:35:54.714425Z",
     "start_time": "2025-03-20T15:35:54.712133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#|export\n",
    "def rank1_decomposition(tensor: torch.Tensor, num_iter: int = 1000, eps: float = 1e-10) -> List[torch.Tensor]:\n",
    "    t_shape = tensor.shape\n",
    "    k = len(t_shape)\n",
    "    decomposed_vecs = [torch.randn(d, dtype=tensor.dtype) for d in t_shape]\n",
    "    decomposed_vecs = [v / v.norm() for v in decomposed_vecs]\n",
    "    zeta = 1.\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        for idx in range(k):\n",
    "            vs = decomposed_vecs[:idx] + decomposed_vecs[idx + 1:]\n",
    "            outer = outer_product(vs).unsqueeze(idx)\n",
    "            sum_indices = list(range(k))\n",
    "            sum_indices.pop(idx)\n",
    "            vi = (tensor * outer).sum(tuple(sum_indices))\n",
    "            vi /= vi.norm()\n",
    "            decomposed_vecs[idx] = vi\n",
    "\n",
    "        zeta_new = (tensor * outer_product(decomposed_vecs)).sum()\n",
    "        if (zeta_new - zeta).norm() < eps:\n",
    "            break\n",
    "        zeta = zeta_new\n",
    "\n",
    "    return decomposed_vecs"
   ],
   "id": "b684cf708594981d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T15:35:54.741122Z",
     "start_time": "2025-03-20T15:35:54.720104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.randn(2, 3, 4, 5, dtype=torch.float32)\n",
    "decompositions = rank1_decomposition(a)\n",
    "# TODO: to compare with the reference implementation\n",
    "# TODO: to test a complex tensor"
   ],
   "id": "afc3edc0bb3f06ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "359a2e03c0dc44589c9b610892469d0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
