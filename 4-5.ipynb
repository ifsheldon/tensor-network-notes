{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5: 生成式张量网络机器学习及优化算法\n",
    "\n",
    "**ResMPS** : 将张量网络 (MPS) 看作从多个向量到单个向量 (或张量) 的映射，实现分类等预测；\n",
    "\n",
    "**生成式MPS** (generative MPS, 简称GMPS) : 由MPS给出样本的概率分布，实现生成任务。\n",
    "\n",
    "> 核心是波恩概率诠释，见 [2.1](2-1.ipynb)\n",
    "\n",
    "GMPS：将样本出现的概率定义为对该样本进行特征映射后所得量子态与MPS态$|\\phi\\rangle$内积的模方\n",
    "\n",
    "将\"0\"（例如一个白色像素）映射为$|0\\rangle$，将\"1\"（例如一个黑色像素）映射为$|1\\rangle$，则样本\"10101\"被映射为直积态$|10101\\rangle$。对于给定量子态$|\\phi\\rangle$，我们设其为矩阵乘积态，则样本\"10101\"在$|\\phi\\rangle$中出现的概率满足\n",
    "\n",
    "$$p(\\text{\"10101\"}) = \\frac{|\\langle10101|\\phi\\rangle|^2}{\\langle\\phi|\\phi\\rangle}$$\n",
    "\n",
    "![gmps_example](./images/gmps_example.png)\n",
    "\n",
    "> 量子概率性机器学习模型又被称为波恩机\n",
    "\n",
    "## GMPS 训练\n",
    "\n",
    "核心任务：为在保持其量子概率诠释的前提下优化局域张量，使得NLL函数极小\n",
    "\n",
    "损失函数：负对数似然（negative logarithmic likelihood，简称NLL）；\n",
    "\n",
    "在目标概率分布为等概率分布时，NLL与交叉熵（见第三章第3节）仅相差一个常数。\n",
    "\n",
    "具体而言：计算矩阵乘积态对于某个数据集中所有样本的概率分布，并训练矩阵乘积态（即优化局域张量），使得各个样本在矩阵乘积态中出现的概率接近其在数据集中出现的概率。一般而言，各个样本在数据集出现的次数为1，因此，我们需要优化局域张量，使得矩阵乘积态给出的各个样本的概率相等。\n",
    "\n",
    "> NLL函数定义：\n",
    "> \n",
    "> $$f = -\\frac{1}{M}\\sum_{m=0}^{M-1}\\ln\\frac{|\\langle\\phi^{(m)}|\\phi\\rangle|^2}{(\\phi|\\phi)} = -\\frac{1}{M}\\sum_{m=0}^{M-1}[\\ln|\\langle\\phi^{(m)}|\\phi\\rangle|^2-\\ln(\\phi|\\phi)]$$\n",
    "\n",
    "Reference:\n",
    "* [Supervised Learning with Tensor Networks](https://papers.nips.cc/paper_files/paper/2016/hash/5314b9674c86e3f9d1ba25ef9bb32895-Abstract.html)\n",
    "\n",
    "### 张量网络的梯度计算\n",
    "TODO: Add Figure\n",
    "\n",
    "例子：\n",
    "![tensor_network_gradient_calculation_example](./images/tensor_network_gradient_calculation_example.png)\n",
    "\n",
    "> * 梯度的第一项对应于 NLL 的第二项，梯度的第二项对应于 NLL 的第一项，因为合并了最前面的负号\n",
    "> * 梯度第一项的形式需要中心正交形式，例子里的 MPS 的中心必须是在第 2 号局域张量\n",
    "\n",
    "计算的时候可以从两端向正交中心迭代计算，好处是梯度计算公式的上下部分可以复用部分计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:33:50.809089Z",
     "start_time": "2025-05-13T16:33:50.133633Z"
    }
   },
   "outputs": [],
   "source": [
    "# |default_exp algorithms.gmps\n",
    "# |export\n",
    "import torch\n",
    "from tensor_network.mps.modules import MPS, MPSType\n",
    "from einops import einsum\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:33:52.865784Z",
     "start_time": "2025-05-13T16:33:52.855189Z"
    }
   },
   "outputs": [],
   "source": [
    "# |export\n",
    "def train_gmps(\n",
    "    *,\n",
    "    samples: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    mps: MPS,\n",
    "    sweep_times: int,\n",
    "    lr: float,\n",
    "    device: torch.device,\n",
    "    enable_tsgo: bool,\n",
    ") -> Tuple[torch.Tensor, MPS]:\n",
    "    assert samples.ndim == 3  # (batch, feature_num, feature_dim)\n",
    "\n",
    "    def samples_at(idx):\n",
    "        return samples[:, idx, :]  # (batch, feature_dim)\n",
    "\n",
    "    EPS = 1e-10\n",
    "    batch_size, feature_num, feature_dim = samples.shape\n",
    "    assert feature_num == mps.length\n",
    "    # set default device to device\n",
    "    prev_device = torch.get_default_device()\n",
    "    torch.set_default_device(device)\n",
    "    # prepare mps, normalize first to avoid numerical instability\n",
    "    mps.normalize_()\n",
    "    mps.center_orthogonalization_(0, mode=\"qr\")\n",
    "    mps_local_tensors = mps._mps  # CAREFUL for inplace operation\n",
    "    # prepare aux variables\n",
    "    norm_factors = torch.ones(batch_size, feature_num)\n",
    "    env_vectors_left = [None] * mps.length\n",
    "    left_virtual_dim = mps_local_tensors[0].shape[0]\n",
    "    env_vectors_left[0] = torch.ones(batch_size, left_virtual_dim)\n",
    "    env_vectors_right = [None] * mps.length\n",
    "    right_virtual_dim = mps_local_tensors[-1].shape[-1]\n",
    "    env_vectors_right[-1] = torch.ones(batch_size, right_virtual_dim)\n",
    "\n",
    "    # prepare env vectors from left to right\n",
    "    def calc_left_to_right_step(current_tensor, current_env_vector_left, current_sample):\n",
    "        next_env_vector_left = einsum(\n",
    "            current_env_vector_left,\n",
    "            current_sample,\n",
    "            current_tensor,\n",
    "            \"batch left, batch physical, left physical right -> batch right\",\n",
    "        )\n",
    "        current_norm_factor = next_env_vector_left.norm(dim=1, keepdim=True)\n",
    "        return next_env_vector_left / (current_norm_factor + EPS), current_norm_factor.squeeze(-1)\n",
    "\n",
    "    for idx in range(mps.center):\n",
    "        next_env_vector_left, current_norm_factor = calc_left_to_right_step(\n",
    "            mps_local_tensors[idx],\n",
    "            env_vectors_left[idx],\n",
    "            samples_at(idx),\n",
    "        )\n",
    "        # set the norm factor for current position to be the norm of the next env vector, since the next env vector is to be normalized\n",
    "        norm_factors[:, idx] = current_norm_factor\n",
    "        env_vectors_left[idx + 1] = next_env_vector_left\n",
    "\n",
    "    # prepare env vectors from right to left\n",
    "    def calc_right_to_left_step(current_tensor, current_env_vector_right, current_sample):\n",
    "        next_env_vector_right = einsum(\n",
    "            current_env_vector_right,\n",
    "            current_sample,\n",
    "            current_tensor,\n",
    "            \"batch right, batch physical, left physical right -> batch left\",\n",
    "        )\n",
    "        current_norm_factor = next_env_vector_right.norm(dim=1, keepdim=True)\n",
    "        return next_env_vector_right / (current_norm_factor + EPS), current_norm_factor.squeeze(-1)\n",
    "\n",
    "    for idx in range(mps.length - 1, mps.center, -1):\n",
    "        next_env_vector_right, current_norm_factor = calc_right_to_left_step(\n",
    "            mps_local_tensors[idx],\n",
    "            env_vectors_right[idx],\n",
    "            samples_at(idx),\n",
    "        )\n",
    "        assert not current_norm_factor.min().isnan()\n",
    "        norm_factors[:, idx] = current_norm_factor\n",
    "        env_vectors_right[idx - 1] = next_env_vector_right\n",
    "\n",
    "    # update the norm factor at the center\n",
    "    norm_factors[:, mps.center] = einsum(\n",
    "        mps_local_tensors[mps.center],\n",
    "        env_vectors_left[mps.center],\n",
    "        samples[:, mps.center, :],\n",
    "        env_vectors_right[mps.center],\n",
    "        \"left physical right, batch left, batch physical, batch right -> batch\",\n",
    "    )\n",
    "\n",
    "    def calc_nll(norm_factors):\n",
    "        nll = -2 * torch.log(norm_factors.abs() + EPS).sum(dim=1)  # (batch)\n",
    "        nll_avg = nll.mean()\n",
    "        return nll_avg\n",
    "\n",
    "    init_nll = calc_nll(norm_factors)\n",
    "\n",
    "    nll_losses = [init_nll]\n",
    "    print(f\"Initially, nll = {init_nll}\")\n",
    "\n",
    "    @torch.compile\n",
    "    def calc_gradient(\n",
    "        env_left_vector, env_right_vector, current_sample, current_tensor, enable_tsgo\n",
    "    ):\n",
    "        raw_grad = einsum(\n",
    "            env_left_vector,\n",
    "            current_sample,\n",
    "            env_right_vector,\n",
    "            \"batch left, batch physical, batch right -> batch right physical left\",\n",
    "        )\n",
    "        norm = einsum(\n",
    "            current_tensor,\n",
    "            raw_grad,\n",
    "            \"left physical right, batch right physical left -> batch\",\n",
    "        )\n",
    "        norm += torch.sign(norm) * EPS  # add a small number to avoid division by zero\n",
    "        grad = (raw_grad / norm.view(-1, 1, 1, 1)).mean(dim=0)\n",
    "        grad = 2 * (current_tensor - grad)\n",
    "        grad_shape = grad.shape\n",
    "        assert grad_shape == current_tensor.shape\n",
    "        if enable_tsgo:\n",
    "            grad = grad.flatten()\n",
    "            current_tensor = current_tensor.flatten()\n",
    "            projection = torch.dot(grad, current_tensor) * current_tensor\n",
    "            grad = grad - projection\n",
    "            grad = grad.reshape(grad_shape)\n",
    "\n",
    "        grad /= grad.norm()\n",
    "        return grad\n",
    "\n",
    "    progress_bar = tqdm(range(sweep_times))\n",
    "    for i in progress_bar:\n",
    "        for idx in range(mps.length):\n",
    "            assert idx == mps.center\n",
    "            grad = calc_gradient(\n",
    "                env_vectors_left[idx],\n",
    "                env_vectors_right[idx],\n",
    "                samples_at(idx),\n",
    "                mps_local_tensors[idx],\n",
    "                enable_tsgo,\n",
    "            )\n",
    "            # update the tensor with gradient, inplace operation, will change mps._mps\n",
    "            mps_local_tensors[idx] -= lr * grad\n",
    "\n",
    "            # prepare for the next iteration\n",
    "            if idx < mps.length - 1:\n",
    "                # move the center to the right\n",
    "                # the local tensors (mps._mps) at idx and idx + 1 will be changed\n",
    "                mps.center_orthogonalization_(idx + 1, mode=\"qr\")\n",
    "                # so we need to update aux variables, only env_vectors_left affected\n",
    "                new_next_env_vector_left, new_norm_factor = calc_left_to_right_step(\n",
    "                    mps_local_tensors[idx],\n",
    "                    env_vectors_left[idx],\n",
    "                    samples_at(idx),\n",
    "                )\n",
    "                env_vectors_left[idx + 1] = new_next_env_vector_left\n",
    "                norm_factors[:, idx] = new_norm_factor\n",
    "            else:\n",
    "                # same as normalize the center tensor, as the center tensor is already at this place\n",
    "                # we need normalization here to make mps as a unit norm state so to preserve the probability interpretation\n",
    "                mps.normalize_()\n",
    "\n",
    "        for idx in range(mps.length - 1, -1, -1):\n",
    "            assert idx == mps.center\n",
    "            grad = calc_gradient(\n",
    "                env_vectors_left[idx],\n",
    "                env_vectors_right[idx],\n",
    "                samples_at(idx),\n",
    "                mps_local_tensors[idx],\n",
    "                enable_tsgo,\n",
    "            )\n",
    "            mps_local_tensors[idx] -= lr * grad\n",
    "\n",
    "            # prepare for the next iteration\n",
    "            if idx > 0:\n",
    "                # move the center to the left\n",
    "                # the local tensors (mps._mps) at idx and idx - 1 will be changed\n",
    "                mps.center_orthogonalization_(idx - 1, mode=\"qr\")\n",
    "                # so we need to update aux variables, only env_vectors_right affected\n",
    "                new_next_env_vector_right, new_norm_factor = calc_right_to_left_step(\n",
    "                    mps_local_tensors[idx],\n",
    "                    env_vectors_right[idx],\n",
    "                    samples_at(idx),\n",
    "                )\n",
    "                env_vectors_right[idx - 1] = new_next_env_vector_right\n",
    "                norm_factors[:, idx] = new_norm_factor\n",
    "            else:\n",
    "                # same as normalize the center tensor, as the center tensor is already at this place\n",
    "                # we need normalization here to make mps as a unit norm state so to preserve the probability interpretation\n",
    "                mps.normalize_()\n",
    "\n",
    "        assert mps.center == 0\n",
    "        # update the norm factor at the center\n",
    "        norm_factors[:, mps.center] = einsum(\n",
    "            mps_local_tensors[mps.center],\n",
    "            env_vectors_left[mps.center],\n",
    "            samples[:, mps.center, :],\n",
    "            env_vectors_right[mps.center],\n",
    "            \"left physical right, batch left, batch physical, batch right -> batch\",\n",
    "        )\n",
    "        nll_loss = calc_nll(norm_factors)\n",
    "        nll_losses.append(nll_loss)\n",
    "        progress_bar.set_description(f\"Iter {i} NLL: {nll_loss:.4f}\")\n",
    "\n",
    "    # restore the default device\n",
    "    torch.set_default_device(prev_device)\n",
    "    return torch.stack(nll_losses), mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:34:16.865851Z",
     "start_time": "2025-05-13T16:34:16.138893Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensor_network.utils.data import get_fashion_mnist_datasets\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tensor_network.feature_mapping import cossin_feature_map\n",
    "\n",
    "cwd = os.getcwd()\n",
    "cache_path = os.path.join(cwd, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:34:18.901129Z",
     "start_time": "2025-05-13T16:34:17.724184Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set, test_set = get_fashion_mnist_datasets(cache_path)\n",
    "t_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
    "train_data, train_labels = next(iter(t_loader))\n",
    "t_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
    "test_data, test_labels = next(iter(t_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:34:20.469914Z",
     "start_time": "2025-05-13T16:34:20.467098Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = torch.device(\"cpu\")\n",
    "batch_size = 2000\n",
    "lr = 1e-1\n",
    "sweep_times = 100\n",
    "\n",
    "feature_dim = 2\n",
    "feature_num = 28 * 28\n",
    "virtual_dim = 50\n",
    "\n",
    "train_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:34:25.067041Z",
     "start_time": "2025-05-13T16:34:25.042041Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data[train_labels == train_class]\n",
    "train_data = train_data.reshape(train_data.shape[0], -1).to(device)\n",
    "train_data = cossin_feature_map(train_data)\n",
    "test_data = test_data[test_labels == train_class]\n",
    "test_data = test_data.reshape(test_data.shape[0], -1).to(device)\n",
    "test_data = cossin_feature_map(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:34:27.370454Z",
     "start_time": "2025-05-13T16:34:27.324689Z"
    }
   },
   "outputs": [],
   "source": [
    "mps = MPS(\n",
    "    length=feature_num,\n",
    "    physical_dim=feature_dim,\n",
    "    virtual_dim=virtual_dim,\n",
    "    mps_type=MPSType.Periodic,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    "    requires_grad=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:35:39.067242Z",
     "start_time": "2025-05-13T16:34:36.201843Z"
    }
   },
   "outputs": [],
   "source": [
    "losses, mps = train_gmps(\n",
    "    samples=train_data,\n",
    "    mps=mps,\n",
    "    sweep_times=sweep_times,\n",
    "    lr=lr,\n",
    "    device=device,\n",
    "    enable_tsgo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
