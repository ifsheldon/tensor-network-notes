{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2：时间演化块消减算法\n",
    "\n",
    "Time-Evolving Block Decimation (TEBD)\n",
    "\n",
    "> References:\n",
    "> * _Efficient classical simulation of slightly entangled quantum computations_\n",
    "> * _Efficient simulation of one-dimensional quantum many-body systems_\n",
    "> * _Infinite time-evolving block decimation algorithm beyond unitary evolution_\n",
    "\n",
    "步骤：\n",
    "1. 随机初始化MPS：$|\\varphi\\rangle$；\n",
    "2. 计算虚时长度为$\\tau$的局域虚时演化算符：$e^{-\\tau\\hat{H}(p)}$；\n",
    "3. 演化MPS并归一化：$e^{-\\tau\\hat{H}}|\\varphi\\rangle \\rightarrow |\\varphi\\rangle/Z$（$Z = \\langle\\varphi|\\varphi\\rangle$）：\n",
    "   - 若MPS虚拟维数超过阈值（截断维数），则利用中心正交形式实现最优裁剪；\n",
    "4. 若$|\\varphi\\rangle$收敛：\n",
    "   a. 若$\\tau$足够小，则返回计算结果，计算结束。\n",
    "   b. 否则，减小$\\tau$并回到步骤2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算技巧\n",
    "\n",
    "### 将二体演化门写成两个三阶张量的缩并，方便后续计算\n",
    "\n",
    "![tebd_evo_gate](./images/tebd_evo_gate.png)\n",
    "\n",
    "不分解直接演化会导致 MPS 张量个数发生改变（如下图）；考虑到程序在处理长程耦合的通用性，我们这里不选择这种方式\n",
    "\n",
    "\n",
    "![tebd_evo_step](./images/tebd_evo_step.png)\n",
    "\n",
    "对演化算符分解后方便处理长程相互作用：演化前后MPS长度不变；\n",
    "\n",
    "演化会破坏对应张量的正交性：对于第$l_1$与第$l_2$个自旋上的局域演化门，\n",
    "\n",
    "作用之后，$l_{\\text{L}}$与$l_{\\text{R}}$上及其之间的所有张量失去正交性\n",
    "\n",
    "![tebd_evo_step_detail](./images/tebd_evo_step_detailed.png)\n",
    "\n",
    "> 演化区域中，张量失去正交性且虚拟指标维数增大\n",
    "\n",
    "对于上图的左边来说\n",
    "\n",
    "![tebd_evo_step_left](./images/tebd_evo_step_left.png)\n",
    "\n",
    "> 缩并之后把 `g` 和 `b` 维度合并在一起\n",
    ">\n",
    "> 表达式是 $A'_{as(gb)} = \\sum_{s'} A_{as'b}L_{sgs'}$\n",
    ">\n",
    "\n",
    "对于上图的中间张量来说\n",
    "\n",
    "![tebd_evo_step_middle](./images/tebd_evo_step_middle.png)\n",
    "\n",
    "> 相当于是张量和 $I$ 做直积之后合并维度，也就是\n",
    ">\n",
    "> $A'_{(ga)s(g'b)} = A_{asb}I_{gg'}$\n",
    ">\n",
    "\n",
    "对于上图的右边来说\n",
    "\n",
    "![tebd_evo_step_right](./images/tebd_evo_step_right.png)\n",
    "\n",
    "> 缩并之后把 `g` 和 `a` 维度合并在一起，相当于是\n",
    ">\n",
    "> $A^{\\prime}_{(ga)sb} = \\sum_{s^{\\prime}} A_{as^{\\prime}b}R_{sgs^{\\prime}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import einsum, rearrange\n",
    "from itertools import permutations\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim=2, einsum_time=0.6932711601257324s, no_calculation_time=0.019344091415405273s, diff=-0.6739270687103271s\n",
      "dim=4, einsum_time=0.010883092880249023s, no_calculation_time=0.021711111068725586s, diff=0.010828018188476562s\n",
      "dim=8, einsum_time=0.014249086380004883s, no_calculation_time=0.12641692161560059s, diff=0.1121678352355957s\n",
      "dim=16, einsum_time=0.02582097053527832s, no_calculation_time=0.17697620391845703s, diff=0.1511552333831787s\n",
      "dim=32, einsum_time=0.058001041412353516s, no_calculation_time=0.46379804611206055s, diff=0.40579700469970703s\n",
      "dim=64, einsum_time=0.12069201469421387s, no_calculation_time=1.3788259029388428s, diff=1.258133888244629s\n",
      "dim=128, einsum_time=0.36752891540527344s, no_calculation_time=4.834371089935303s, diff=4.466842174530029s\n"
     ]
    }
   ],
   "source": [
    "tries = 1000\n",
    "\n",
    "# try two different ways to calculate the direct product of I and a\n",
    "# Surprisingly, the einsum way is faster than the way in which no calculation is needed\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    I = torch.eye(dim, dtype=torch.int32)\n",
    "    a = torch.randn(3, 5, 7, dtype=torch.float32)\n",
    "    start = time()\n",
    "    for _ in range(tries):\n",
    "        Ia_einsum = einsum(I, a, \"g0 g1, a s b -> g0 a s g1 b\")\n",
    "        Ia_einsum = rearrange(Ia_einsum, \"g0 a s g1 b -> (g0 a) s (g1 b)\")\n",
    "    end = time()\n",
    "    einsum_time = end - start\n",
    "\n",
    "    start = time()\n",
    "    for _ in range(tries):\n",
    "        a_ = a.unsqueeze(0).unsqueeze(0).repeat(dim, dim, 1, 1, 1)\n",
    "        perms = permutations(range(dim), r=2)\n",
    "        x, y = zip(*perms)\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        a_[x, y] = 0.0\n",
    "        a_[y, x] = 0.0\n",
    "        Ia_no_calculation = rearrange(a_, \"g0 g1 a s b -> (g0 a) s (g1 b)\")\n",
    "    end = time()\n",
    "    no_calculation_time = end - start\n",
    "    assert torch.allclose(Ia_no_calculation, Ia_einsum)\n",
    "\n",
    "    print(\n",
    "        f\"dim={dim}, {einsum_time=}s, {no_calculation_time=}s, diff={no_calculation_time - einsum_time}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将二体演化算符写成两个三阶张量的收缩，在这里，我们采取一种简单的分解方式，即\n",
    "\n",
    "$G_{abcd} \\stackrel{\\text{permute}}{\\longrightarrow} G_{abdc} \\stackrel{\\text{reshape (bd)}\\rightarrow g}{\\longrightarrow} (\\text{gl})_{agc}$\n",
    "\n",
    "$I_{bb'}I_{dd'} \\stackrel{\\otimes}{\\longrightarrow} (gr)_{bb'dd'} \\stackrel{\\text{permute}}{\\longrightarrow} (\\text{gr})_{bb'd'd} \\stackrel{\\text{reshape (b'd')}\\rightarrow g}{\\longrightarrow} (\\text{gr})_{bgd}$\n",
    "\n",
    "也就是如下图\n",
    "\n",
    "![tebd_op_to_tensors](./images/tebd_op_to_tensors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = torch.randn(2, 2, 2, 2, dtype=torch.complex64)\n",
    "I = torch.eye(2, dtype=torch.int32)\n",
    "\n",
    "gr = einsum(I, I, \"b0 b1, d0 d1 -> b0 b1 d0 d1\")\n",
    "gr = rearrange(gr, \"b0 b1 d0 d1 -> b0 (b1 d1) d0\")\n",
    "\n",
    "gl = rearrange(op, \"a b c d -> a (b d) c\")\n",
    "\n",
    "product = einsum(gl, gr.to(gl.dtype), \"a g c , b g d -> a b c d\")\n",
    "\n",
    "assert torch.allclose(product, op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尽量少地移动正交中心；保持裁剪的最优性\n",
    "\n",
    "策略要点：尽量少地移动正交中心；保持裁剪的最优性\n",
    "\n",
    "1. 将正交中心以移动至$l_L$与$l_R$二者中距离当前正交中心最近的位置；\n",
    "2. 演化后，从$l_L$或$l_R$出发，对整个演化区地张量做正交化变换（不裁剪）；\n",
    "3. 当MPS处于正交中心形式后，通过移动中心的奇异值分解进行虚拟维数裁剪；\n",
    "4. 正交化后的正交中心要尽可能地靠近下一次演化格点的位置。\n",
    "\n",
    "> 例：\n",
    "> \n",
    "> 演化前正交中心位于 $l_c = 4$，下一次的演化算符分别作用于自旋 1 与 2，再下一次的演化作用于自旋 5 与 6，则最优的中心正交化策略如下\n",
    ">\n",
    "> ![tebd_center_orthogonalization_example](./images/tebd_center_orthogonalization_example.png)\n",
    ">\n",
    "> 左图正交中心位置变化：4 → 2 → None → 1 → 2\n",
    ">\n",
    "> 2 → None 是演化步骤，1 → 2 是裁剪步骤\n",
    "> \n",
    ">\n",
    "> 下一步演化位置为5与6（若设再下一步演化位置为0和1），则对应的正交中心位置变化：\n",
    "> 2 → 5 → None → 6 → 5\n",
    ">\n",
    "> 5 → None 是演化步骤，6 → 5 是裁剪步骤\n",
    ">\n",
    "\n",
    "强调：非中心正交形式下不裁剪，否则不能达到最优裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： Algorithms/MPS_Algo.py/tebd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
