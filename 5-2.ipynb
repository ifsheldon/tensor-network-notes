{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2：时间演化块消减算法\n",
    "\n",
    "Time-Evolving Block Decimation (TEBD)\n",
    "\n",
    "> References:\n",
    "> * _Efficient classical simulation of slightly entangled quantum computations_\n",
    "> * _Efficient simulation of one-dimensional quantum many-body systems_\n",
    "> * _Infinite time-evolving block decimation algorithm beyond unitary evolution_\n",
    "\n",
    "步骤：\n",
    "1. 随机初始化MPS：$|\\varphi\\rangle$；\n",
    "2. 计算虚时长度为$\\tau$的局域虚时演化算符：$e^{-\\tau\\hat{H}(p)}$；\n",
    "3. 演化MPS并归一化：$e^{-\\tau\\hat{H}}|\\varphi\\rangle \\rightarrow |\\varphi\\rangle/Z$（$Z = \\langle\\varphi|\\varphi\\rangle$）：\n",
    "   - 若MPS虚拟维数超过阈值（截断维数），则利用中心正交形式实现最优裁剪；\n",
    "4. 若$|\\varphi\\rangle$收敛：\n",
    "   1. 若$\\tau$足够小，则返回计算结果，计算结束。\n",
    "   2. 否则，减小$\\tau$并回到步骤2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算技巧\n",
    "\n",
    "### 将二体演化门写成两个三阶张量的缩并，方便后续计算\n",
    "\n",
    "![tebd_evo_gate](./images/tebd_evo_gate.png)\n",
    "\n",
    "不分解直接演化会导致 MPS 张量个数发生改变（如下图）；考虑到程序在处理长程耦合的通用性，我们这里不选择这种方式\n",
    "\n",
    "\n",
    "![tebd_evo_step](./images/tebd_evo_step.png)\n",
    "\n",
    "对演化算符分解后方便处理长程相互作用：演化前后MPS长度不变；\n",
    "\n",
    "演化会破坏对应张量的正交性：对于第$l_1$与第$l_2$个自旋上的局域演化门，\n",
    "\n",
    "作用之后，$l_{\\text{L}}$与$l_{\\text{R}}$上及其之间的所有张量失去正交性\n",
    "\n",
    "![tebd_evo_step_detail](./images/tebd_evo_step_detailed.png)\n",
    "\n",
    "> 演化区域中，张量失去正交性且虚拟指标维数增大\n",
    "\n",
    "对于上图的左边来说\n",
    "\n",
    "![tebd_evo_step_left](./images/tebd_evo_step_left.png)\n",
    "\n",
    "> 缩并之后把 `g` 和 `b` 维度合并在一起\n",
    ">\n",
    "> 表达式是 $A'_{as(gb)} = \\sum_{s'} A_{as'b}L_{sgs'}$\n",
    ">\n",
    "\n",
    "对于上图的中间张量来说\n",
    "\n",
    "![tebd_evo_step_middle](./images/tebd_evo_step_middle.png)\n",
    "\n",
    "> 相当于是张量和 $I$ 做直积之后合并维度，也就是\n",
    ">\n",
    "> $A'_{(ga)s(g'b)} = A_{asb}I_{gg'}$\n",
    ">\n",
    "\n",
    "对于上图的右边来说\n",
    "\n",
    "![tebd_evo_step_right](./images/tebd_evo_step_right.png)\n",
    "\n",
    "> 缩并之后把 `g` 和 `a` 维度合并在一起，相当于是\n",
    ">\n",
    "> $A^{\\prime}_{(ga)sb} = \\sum_{s^{\\prime}} A_{as^{\\prime}b}R_{sgs^{\\prime}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp algorithms.time_evolving_block_decimation\n",
    "# |export\n",
    "import torch\n",
    "from einops import einsum, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim=2, einsum_time=0.8842740058898926s, no_calculation_time=0.02145099639892578s, diff=-0.8628230094909668s\n",
      "dim=4, einsum_time=0.011330842971801758s, no_calculation_time=0.022800207138061523s, diff=0.011469364166259766s\n",
      "dim=8, einsum_time=0.014983892440795898s, no_calculation_time=0.1348409652709961s, diff=0.1198570728302002s\n",
      "dim=16, einsum_time=0.027770042419433594s, no_calculation_time=0.1813349723815918s, diff=0.1535649299621582s\n",
      "dim=32, einsum_time=0.0618901252746582s, no_calculation_time=0.49808788299560547s, diff=0.43619775772094727s\n",
      "dim=64, einsum_time=0.12726569175720215s, no_calculation_time=1.405144214630127s, diff=1.2778785228729248s\n",
      "dim=128, einsum_time=0.37425994873046875s, no_calculation_time=4.831672668457031s, diff=4.4574127197265625s\n"
     ]
    }
   ],
   "source": [
    "tries = 1000\n",
    "skip_perf_benchmark = False\n",
    "\n",
    "# try two different ways to calculate the direct product of I and a\n",
    "# Surprisingly, the einsum way is faster than the way in which no calculation is needed\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    if skip_perf_benchmark:\n",
    "        break\n",
    "    I = torch.eye(dim, dtype=torch.int32)\n",
    "    a = torch.randn(3, 5, 7, dtype=torch.float32)\n",
    "    start = time()\n",
    "    for _ in range(tries):\n",
    "        Ia_einsum = einsum(I, a, \"g0 g1, a s b -> g0 a s g1 b\")\n",
    "        Ia_einsum = rearrange(Ia_einsum, \"g0 a s g1 b -> (g0 a) s (g1 b)\")\n",
    "    end = time()\n",
    "    einsum_time = end - start\n",
    "\n",
    "    start = time()\n",
    "    for _ in range(tries):\n",
    "        a_ = a.unsqueeze(0).unsqueeze(0).repeat(dim, dim, 1, 1, 1)\n",
    "        perms = permutations(range(dim), r=2)\n",
    "        x, y = zip(*perms)\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        a_[x, y] = 0.0\n",
    "        a_[y, x] = 0.0\n",
    "        Ia_no_calculation = rearrange(a_, \"g0 g1 a s b -> (g0 a) s (g1 b)\")\n",
    "    end = time()\n",
    "    no_calculation_time = end - start\n",
    "    assert torch.allclose(Ia_no_calculation, Ia_einsum)\n",
    "\n",
    "    print(\n",
    "        f\"dim={dim}, {einsum_time=}s, {no_calculation_time=}s, diff={no_calculation_time - einsum_time}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将二体演化算符写成两个三阶张量的收缩，在这里，我们采取一种简单的分解方式，即\n",
    "\n",
    "$G_{abcd} \\stackrel{\\text{permute}}{\\longrightarrow} G_{abdc} \\stackrel{\\text{reshape (bd)}\\rightarrow g}{\\longrightarrow} (\\text{gl})_{agc}$\n",
    "\n",
    "$I_{bb'}I_{dd'} \\stackrel{\\otimes}{\\longrightarrow} (gr)_{bb'dd'} \\stackrel{\\text{permute}}{\\longrightarrow} (\\text{gr})_{bb'd'd} \\stackrel{\\text{reshape (b'd')}\\rightarrow g}{\\longrightarrow} (\\text{gr})_{bgd}$\n",
    "\n",
    "也就是如下图\n",
    "\n",
    "![tebd_op_to_tensors](./images/tebd_op_to_tensors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = torch.randn(2, 2, 2, 2, dtype=torch.complex64)\n",
    "I = torch.eye(2, dtype=torch.int32)\n",
    "\n",
    "gr = einsum(I, I, \"b0 b1, d0 d1 -> b0 b1 d0 d1\")\n",
    "gr = rearrange(gr, \"b0 b1 d0 d1 -> b0 (b1 d1) d0\")\n",
    "\n",
    "gl = rearrange(op, \"a b c d -> a (b d) c\")\n",
    "\n",
    "product = einsum(gl, gr.to(gl.dtype), \"a g c , b g d -> a b c d\")\n",
    "\n",
    "assert torch.allclose(product, op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尽量少地移动正交中心；保持裁剪的最优性\n",
    "\n",
    "策略要点：尽量少地移动正交中心；保持裁剪的最优性\n",
    "\n",
    "1. 将正交中心以移动至$l_L$与$l_R$二者中距离当前正交中心最近的位置；\n",
    "2. 演化后，从$l_L$或$l_R$出发，对整个演化区地张量做正交化变换（不裁剪）；\n",
    "3. 当MPS处于正交中心形式后，通过移动中心的奇异值分解进行虚拟维数裁剪；\n",
    "4. 正交化后的正交中心要尽可能地靠近下一次演化格点的位置。\n",
    "\n",
    "> 例：\n",
    "> \n",
    "> 演化前正交中心位于 $l_c = 4$，下一次的演化算符分别作用于自旋 1 与 2，再下一次的演化作用于自旋 5 与 6，则最优的中心正交化策略如下\n",
    ">\n",
    "> ![tebd_center_orthogonalization_example](./images/tebd_center_orthogonalization_example.png)\n",
    ">\n",
    "> 左图正交中心位置变化：4 → 2 → None → 1 → 2\n",
    ">\n",
    "> 2 → None 是演化步骤，1 → 2 是裁剪步骤\n",
    "> \n",
    ">\n",
    "> 下一步演化位置为5与6（若设再下一步演化位置为0和1），则对应的正交中心位置变化：\n",
    "> 2 → 5 → None → 6 → 5\n",
    ">\n",
    "> 5 → None 是演化步骤，6 → 5 是裁剪步骤\n",
    ">\n",
    "\n",
    "强调：非中心正交形式下不裁剪，否则不能达到最优裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from typing import List\n",
    "from tensor_network.mps.modules import MPS\n",
    "from tensor_network.utils.mapping import view_gate_matrix_as_tensor, view_gate_tensor_as_matrix\n",
    "from tensor_network.utils.checking import check_quantum_gate\n",
    "\n",
    "\n",
    "def evolve_gate_2body(\n",
    "    mps_local_tensors: List[torch.Tensor], gl: torch.Tensor, gr: torch.Tensor, p0: int, p1: int\n",
    "):\n",
    "    # gl - (a, g, c)\n",
    "    # gr - (b, g, d)\n",
    "    assert p0 < p1\n",
    "    assert gl.shape[1] == gr.shape[1]\n",
    "    g_dim = gl.shape[1]\n",
    "    local_tensors = mps_local_tensors\n",
    "    local_tensor_left = local_tensors[p0]\n",
    "    local_tensor_left = einsum(\n",
    "        local_tensor_left,\n",
    "        gl,\n",
    "        \"left physical_c right, new_physical g physical_c -> left new_physical g right\",\n",
    "    )\n",
    "    local_tensors[p0] = rearrange(\n",
    "        local_tensor_left, \"left new_physical g right -> left new_physical (g right)\"\n",
    "    )\n",
    "\n",
    "    local_tensor_right = local_tensors[p1]\n",
    "    local_tensor_right = einsum(\n",
    "        local_tensor_right,\n",
    "        gr,\n",
    "        \"left physical right, new_physical g physical -> g left new_physical right\",\n",
    "    )\n",
    "    local_tensors[p1] = rearrange(\n",
    "        local_tensor_right, \"g left new_physical right -> (g left) new_physical right\"\n",
    "    )\n",
    "\n",
    "    I = torch.eye(g_dim, dtype=torch.int32, device=mps_local_tensors[0].device)\n",
    "\n",
    "    for idx in range(p0 + 1, p1):\n",
    "        local_tensor_i = local_tensors[idx]\n",
    "        new_local_tensor_i = einsum(\n",
    "            I, local_tensor_i, \"g0 g1, left physical right -> g0 left physical g1 right\"\n",
    "        )\n",
    "        local_tensors[idx] = rearrange(\n",
    "            new_local_tensor_i, \"g0 left physical g1 right -> (g0 left) physical (g1 right)\"\n",
    "        )\n",
    "\n",
    "    return local_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test evolve_gate_2body\n",
    "from tensor_network.tensor_gates.functional import apply_gate\n",
    "from tensor_network.mps.modules import MPSType\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "skip_test_evolve_gate_2body = False\n",
    "\n",
    "for _ in range(10):\n",
    "    if skip_test_evolve_gate_2body:\n",
    "        break\n",
    "    for length in range(2, 10):\n",
    "        mps = MPS(\n",
    "            length=length,\n",
    "            physical_dim=2,\n",
    "            virtual_dim=10,\n",
    "            mps_type=MPSType.Open,\n",
    "            dtype=torch.complex128,\n",
    "            device=cpu,\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        mps.center_orthogonalization_(0, mode=\"qr\", normalize=True)\n",
    "        mps.normalize_()\n",
    "        global_tensor = mps.global_tensor()\n",
    "        gate = torch.randn(2, 2, 2, 2, dtype=mps.dtype, device=cpu)\n",
    "        gate = gate / gate.norm()\n",
    "        I = torch.eye(mps.physical_dim, dtype=torch.int32, device=cpu)\n",
    "        gr = einsum(I, I, \"b0 b1, d0 d1 -> b0 b1 d0 d1\")\n",
    "        gr = rearrange(gr, \"b0 b1 d0 d1 -> b0 (b1 d1) d0\").to(dtype=mps.dtype)\n",
    "        gl = rearrange(gate, \"a b c d -> a (b d) c\")\n",
    "        for target_qubits in combinations(range(mps.length), 2):\n",
    "            new_mps_local_tensors = evolve_gate_2body(\n",
    "                mps.local_tensors, gl, gr, target_qubits[0], target_qubits[1]\n",
    "            )\n",
    "            new_global_tensor = MPS(mps_tensors=new_mps_local_tensors).global_tensor()\n",
    "            global_tensor_ref = apply_gate(\n",
    "                quantum_state=global_tensor, gate=gate, target_qubit=list(target_qubits)\n",
    "            )\n",
    "            assert torch.allclose(new_global_tensor, global_tensor_ref), f\"\"\"\n",
    "{length=}\n",
    "{new_global_tensor.flatten()}\n",
    "{global_tensor_ref.flatten()}\n",
    "L2-Norm = {torch.norm(new_global_tensor - global_tensor_ref)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.basics import patch\n",
    "from tensor_network.quantum_state.functional import calc_reduced_density_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export mps.modules\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "@patch\n",
    "def two_body_reduced_density_matrix_(\n",
    "    self: MPS, qubit_idx0: int, qubit_idx1: int, return_matrix: bool = False\n",
    ") -> torch.Tensor:\n",
    "    assert 0 <= qubit_idx0 < qubit_idx1\n",
    "    self.center_orthogonalization_(qubit_idx0, mode=\"qr\", normalize=True)\n",
    "\n",
    "    tensor_left = self._mps[qubit_idx0]\n",
    "    product = einsum(\n",
    "        tensor_left.conj(),\n",
    "        tensor_left,\n",
    "        \"left physical_conj right_conj, left physical right -> physical_conj physical right_conj right\",\n",
    "    )\n",
    "\n",
    "    for idx in range(qubit_idx0 + 1, qubit_idx1):\n",
    "        tensor_i = self._mps[idx]\n",
    "        product = einsum(\n",
    "            product,\n",
    "            tensor_i.conj(),\n",
    "            tensor_i,\n",
    "            \"i0_physical_conj i0_physical left_conj left, left_conj physical right_conj, left physical right -> i0_physical_conj i0_physical right_conj right\",\n",
    "        )\n",
    "\n",
    "    tensor_right = self._mps[qubit_idx1]\n",
    "    rdm = einsum(\n",
    "        product,\n",
    "        tensor_right.conj(),\n",
    "        tensor_right,\n",
    "        \"i0_physical_conj i0_physical left_conj left, left_conj i1_physical_conj right, left i1_physical right -> i0_physical i1_physical i0_physical_conj i1_physical_conj \",\n",
    "    )\n",
    "\n",
    "    if return_matrix:\n",
    "        return rearrange(rdm, \"a b c d -> (a b) (c d)\")\n",
    "    else:\n",
    "        return rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test two_body_reduced_density_matrix_\n",
    "cpu = torch.device(\"cpu\")\n",
    "length = 4\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "skip_test_two_body_reduced_density_matrix = False\n",
    "\n",
    "for _ in range(5):\n",
    "    if skip_test_two_body_reduced_density_matrix:\n",
    "        break\n",
    "    for length in range(2, 10):\n",
    "        mps = MPS(\n",
    "            length=length,\n",
    "            physical_dim=2,\n",
    "            virtual_dim=10,\n",
    "            mps_type=MPSType.Open,\n",
    "            dtype=torch.complex128,\n",
    "            device=cpu,\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        mps.center_orthogonalization_(0, mode=\"qr\", normalize=True)\n",
    "        for target_qubits in combinations(range(mps.length), 2):\n",
    "            rdm = mps.two_body_reduced_density_matrix_(\n",
    "                target_qubits[0], target_qubits[1], return_matrix=False\n",
    "            )\n",
    "\n",
    "            rdm0 = einsum(rdm, \"ket0 kb1 bra0 kb1 -> ket0 bra0\")\n",
    "            rdm0_ref = mps.one_body_reduced_density_matrix(\n",
    "                idx=target_qubits[0], do_tracing=False, inplace_mutation=False\n",
    "            )\n",
    "            assert torch.allclose(rdm0, rdm0_ref)\n",
    "\n",
    "            global_tensor = mps.global_tensor()\n",
    "            rdm_mat = rearrange(rdm, \"a b c d -> (a b) (c d)\")\n",
    "            rdm_ref = calc_reduced_density_matrix(global_tensor, list(target_qubits))\n",
    "            assert torch.allclose(rdm_mat, rdm_ref), f\"rdm: {rdm}, rdm_ref: {rdm_ref}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def direction_to_next_center(\n",
    "    l0: int, r0: int, l1: int, r1: int\n",
    ") -> Literal[\"right-to-left\", \"left-to-right\"]:\n",
    "    l_min = min([abs(l0 - l1), abs(l0 - r1)])\n",
    "    r_min = min([abs(r0 - l1), abs(r0 - r1)])\n",
    "    if l_min < r_min:\n",
    "        return \"right-to-left\"\n",
    "    else:\n",
    "        return \"left-to-right\"\n",
    "\n",
    "\n",
    "def calculate_mps_local_energies(\n",
    "    mps: MPS, hamiltonians: List[torch.Tensor], positions: List[List[int]] | torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    assert len(hamiltonians) == len(positions)\n",
    "    local_energies = []\n",
    "    for pos, hamiltonian in zip(positions, hamiltonians):\n",
    "        assert len(pos) == 2, \"Only support 2-body interaction for now\"\n",
    "        rdm = mps.two_body_reduced_density_matrix_(pos[0], pos[1], return_matrix=True)\n",
    "        local_energies.append(einsum(hamiltonian, rdm, \"a b, b a ->\"))\n",
    "\n",
    "    return torch.stack(local_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from tensor_network.mps.functional import orthogonalize_arange\n",
    "from typing import Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def _prepare_gates(\n",
    "    hamiltonians: List[torch.Tensor], tau: float, interaction_num: int\n",
    ") -> List[torch.Tensor]:\n",
    "    gates = [\n",
    "        view_gate_matrix_as_tensor(\n",
    "            torch.matrix_exp(-tau * view_gate_tensor_as_matrix(h)) for h in hamiltonians\n",
    "        )\n",
    "    ]\n",
    "    if len(gates) == 1:\n",
    "        gates = gates * interaction_num\n",
    "\n",
    "    return gates\n",
    "\n",
    "\n",
    "def tebd(\n",
    "    hamiltonians: torch.Tensor | List[torch.Tensor],\n",
    "    positions: torch.Tensor | List[List[int]],\n",
    "    mps: MPS,\n",
    "    tau: float,\n",
    "    iterations: int,\n",
    "    calc_observation_iters: int,\n",
    "    e0_eps: float,\n",
    "    tau_min: float,\n",
    "    least_iters_for_tau: int,\n",
    "    max_virtual_dim: int,\n",
    "    progress_bar_kwargs: dict = {},\n",
    ") -> Tuple[MPS, torch.Tensor]:\n",
    "    device = mps.device\n",
    "    dtype = mps.dtype\n",
    "\n",
    "    if isinstance(positions, List):\n",
    "        positions = torch.tensor(positions, device=device, dtype=torch.long)\n",
    "    else:\n",
    "        assert isinstance(positions, torch.Tensor)\n",
    "    assert torch.all(positions[:, 0] < positions[:, 1])\n",
    "\n",
    "    assert positions.ndim == 2  # (interaction_num, gate_apply_qubit_num)\n",
    "    interaction_num, gate_apply_qubit_num = positions.shape\n",
    "    assert gate_apply_qubit_num == 2, \"Only support 2-body interaction for now\"\n",
    "    if isinstance(hamiltonians, torch.Tensor):\n",
    "        if hamiltonians.ndim == 2 * gate_apply_qubit_num:\n",
    "            hamiltonians = [hamiltonians]\n",
    "        elif hamiltonians.ndim == 2 * gate_apply_qubit_num + 1:\n",
    "            assert hamiltonians.shape[0] == interaction_num\n",
    "            hamiltonians = [hamiltonians[i] for i in range(interaction_num)]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid hamiltonians shape: {hamiltonians.shape}\")\n",
    "    elif isinstance(hamiltonians, List):\n",
    "        assert len(hamiltonians) == interaction_num\n",
    "        for h in hamiltonians:\n",
    "            assert h.ndim == 2 * gate_apply_qubit_num\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid hamiltonians type: {type(hamiltonians)}\")\n",
    "\n",
    "    hamiltonians = [h.to(device=device) for h in hamiltonians]\n",
    "    for h in hamiltonians:\n",
    "        check_quantum_gate(h, num_qubits=gate_apply_qubit_num)\n",
    "\n",
    "    assert 1.0 > tau >= 0.0 and 1.0 > tau_min >= 0.0\n",
    "    assert iterations >= 0 and calc_observation_iters >= 0 and least_iters_for_tau >= 1\n",
    "    assert max_virtual_dim >= 1\n",
    "\n",
    "    mps.center_orthogonalization_(\n",
    "        positions[0, -1].item(), mode=\"svd\", truncate_dim=max_virtual_dim, normalize=True\n",
    "    )\n",
    "    mps.normalize_()\n",
    "\n",
    "    gates = _prepare_gates(hamiltonians, tau, interaction_num)\n",
    "\n",
    "    I = torch.eye(mps.physical_dim, dtype=torch.int32, device=device)\n",
    "    gr = einsum(I, I, \"b0 b1, d0 d1 -> b0 b1 d0 d1\")\n",
    "    gr = rearrange(gr, \"b0 b1 d0 d1 -> b0 (b1 d1) d0\").to(dtype=dtype)  # (b, g, d)\n",
    "\n",
    "    inversed_temperatur = 0.0\n",
    "    iters_for_tau = 0\n",
    "    local_energies = 0.0\n",
    "\n",
    "    for t in tqdm(range(iterations), **progress_bar_kwargs):\n",
    "        for p in range(gate_apply_qubit_num):\n",
    "            p_left = positions[p, 0].item()\n",
    "            p_right = positions[p, 1].item()\n",
    "\n",
    "            if abs(mps.center - p_left) < abs(mps.center - p_right):\n",
    "                mps.center_orthogonalization_(p_left, mode=\"qr\")\n",
    "            else:\n",
    "                mps.center_orthogonalization_(p_right, mode=\"qr\")\n",
    "\n",
    "            gate = gates[p]\n",
    "            gl = rearrange(gate, \"a b c d -> a (b d) c\")  # (a, g, c)\n",
    "            new_mps_local_tensors = evolve_gate_2body(mps.local_tensors, gl, gr, p_left, p_right)\n",
    "\n",
    "            if p == gate_apply_qubit_num - 1:\n",
    "                pos_next = positions[0]\n",
    "            else:\n",
    "                pos_next = positions[p + 1]\n",
    "\n",
    "            direction = direction_to_next_center(p_left, p_right, pos_next[0], pos_next[1])\n",
    "            if direction == \"right-to-left\":\n",
    "                new_mps_local_tensors = orthogonalize_arange(\n",
    "                    mps_tensors=mps.local_tensors,\n",
    "                    start_idx=p_right,\n",
    "                    end_idx=p_left,\n",
    "                    mode=\"qr\",\n",
    "                    normalize=False,\n",
    "                )\n",
    "                new_mps_local_tensors = orthogonalize_arange(\n",
    "                    mps_tensors=new_mps_local_tensors,\n",
    "                    start_idx=p_left,\n",
    "                    end_idx=p_right,\n",
    "                    mode=\"svd\",\n",
    "                    truncate_dim=max_virtual_dim,\n",
    "                    normalize=False,\n",
    "                )\n",
    "                mps = MPS(mps_tensors=new_mps_local_tensors)\n",
    "                mps._center = p_right\n",
    "            else:\n",
    "                new_mps_local_tensors = orthogonalize_arange(\n",
    "                    mps_tensors=mps.local_tensors,\n",
    "                    start_idx=p_left,\n",
    "                    end_idx=p_right,\n",
    "                    mode=\"qr\",\n",
    "                    normalize=False,\n",
    "                )\n",
    "                new_mps_local_tensors = orthogonalize_arange(\n",
    "                    mps_tensors=new_mps_local_tensors,\n",
    "                    start_idx=p_right,\n",
    "                    end_idx=p_left,\n",
    "                    mode=\"svd\",\n",
    "                    truncate_dim=max_virtual_dim,\n",
    "                    normalize=False,\n",
    "                )\n",
    "                mps = MPS(mps_tensors=new_mps_local_tensors)\n",
    "                mps._center = p_left\n",
    "            mps.center_normalize_()\n",
    "        inversed_temperatur += tau\n",
    "        iters_for_tau += 1\n",
    "\n",
    "        if iters_for_tau > tau_min and (t + 1) % calc_observation_iters == 0:\n",
    "            local_energies_new = calculate_mps_local_energies(mps, hamiltonians, positions)\n",
    "            avg_diff_local_energies = (local_energies_new - local_energies).abs().mean()\n",
    "            local_energies = local_energies_new\n",
    "            if avg_diff_local_energies < e0_eps:\n",
    "                print(f\"Iteration {t}\")\n",
    "                tau *= 0.5\n",
    "                iters_for_tau = 0\n",
    "                if tau < tau_min:\n",
    "                    print(\"  E converged. Break iteration.\")\n",
    "                    break\n",
    "\n",
    "                gates = _prepare_gates(hamiltonians, tau, interaction_num)\n",
    "                print(f\"  Reduce tau to {tau}\")\n",
    "\n",
    "    return mps, local_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
