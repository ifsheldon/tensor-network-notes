{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6: 生成式张量网络与量子采样\n",
    "\n",
    "核心任务：利用GMPS（$|\\varphi\\rangle$）与量子采样，实现样本生成（generation）\n",
    "\n",
    "1. 对于第m个特征的生成，需依据量子力学原理（参考2.6节）计算边缘概率\n",
    "   $p(x_m) = \\langle\\phi_m|\\hat{\\rho}^{(m)}|\\phi_m\\rangle$\n",
    "   \n",
    "   其中$\\hat{\\rho}^{(m)} = \\text{Tr}_{/m}|\\varphi\\rangle\\langle\\varphi|$，$|\\phi_m\\rangle$为特征$x_m$进行量子特征映射后得到的量子态。\n",
    "2. 得到概率分布$p(x_m)$后，特征$x_m$取值的生成方式有多种，例如可直接根据概率分布进行采样，或计算最概然的取值$\\text{argmax}_{x_m}[p(x_m)]$，或计算$x_m$的概率期望值$\\langle x_m\\rangle_{p(x_m)} = \\int p(x_m)x_m dx_m$等\n",
    "\n",
    "## 投影测量\n",
    "\n",
    "对于已知或已生成的特征，我们对GMPS进行投影测量：\n",
    "\n",
    "$|\\varphi'\\rangle = \\frac{1}{Z}\\langle\\Phi_m|\\varphi\\rangle$\n",
    "\n",
    "张量网络图如下\n",
    "![mps_projection](./images/mps_projection.png)\n",
    "\n",
    "测量后的量子态仍满足玻恩概率诠释，描述其余位置量子态的概率分布\n",
    "\n",
    "### GMPS 生成\n",
    "\n",
    "已知部分特征取值（记其位置的集合为$\\mathcal{A}$）的情况下，利用GMPS对另一部分特征（记其位置的集合为$\\mathcal{B}$）进行生成。我们将除$\\mathcal{A}$与$\\mathcal{B}$外的其余特征位置构成的集合记为$\\mathcal{C}$，即$\\mathcal{A} \\cup \\mathcal{B} \\cup \\mathcal{C}$构成样本全部特征的位置集合\n",
    "\n",
    "测量后量子态的边缘概率为测量前量子态的条件概率：\n",
    "\n",
    "$$p(\\{x_m\\}(m \\in \\mathcal{B})|\\{x_{m'}\\}(m' \\in \\mathcal{A})) = \\left(\\prod_{m\\in\\mathcal{B}}|\\phi_m|\\right)\\hat{\\rho}^{(\\mathcal{B})}\\left(\\prod_{m\\in\\mathcal{B}}|\\phi_m|\\right)$$\n",
    "\n",
    "约化密度矩阵$\\hat{\\rho}^{(B)}$由测量后量子态所得，如下图\n",
    "![mps_reduced_density_matrix](./images/mps_reduced_density_matrix_calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor_network.mps.modules import MPS\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export mps.functional\n",
    "from copy import deepcopy\n",
    "from einops import einsum\n",
    "\n",
    "\n",
    "def project_multi_qubits(\n",
    "    mps_local_tensors: List[torch.Tensor],\n",
    "    qubit_indices: List[int],\n",
    "    project_to_states: torch.Tensor | List[int],\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Do projection of multiple qubits of MPS, giving new local tensors of a new MPS.\n",
    "\n",
    "    Args:\n",
    "        mps_local_tensors: List of local tensors of the MPS.\n",
    "        qubit_indices: List of indices of the qubits to project.\n",
    "        project_to_states: List of states to project to.\n",
    "\n",
    "    Returns:\n",
    "        List of new local tensors of the MPS.\n",
    "    \"\"\"\n",
    "    assert isinstance(project_to_states, (torch.Tensor, List)), \"states must be a tensor or a list\"\n",
    "    qubit_indices = deepcopy(qubit_indices)\n",
    "    project_qubit_num = len(qubit_indices)\n",
    "    if isinstance(project_to_states, torch.Tensor):\n",
    "        assert project_to_states.ndimension() == 2, (\n",
    "            \"states must be a 2D tensor\"\n",
    "        )  # (feature_num, channel)\n",
    "        assert project_to_states.shape[0] == project_qubit_num, (\n",
    "            \"feature_num must match qubit_indices\"\n",
    "        )\n",
    "        is_vec = True\n",
    "    else:\n",
    "        assert isinstance(project_to_states, List), \"project_to_states must be a list\"\n",
    "        assert len(project_to_states) == project_qubit_num, (\n",
    "            \"project_to_states must match qubit_indices\"\n",
    "        )\n",
    "        is_vec = False\n",
    "\n",
    "    local_tensors = [i for i in mps_local_tensors]\n",
    "\n",
    "    if project_qubit_num == 0:\n",
    "        return local_tensors\n",
    "\n",
    "    if is_vec:\n",
    "        for i, qubit_idx in enumerate(qubit_indices):\n",
    "            local_tensor = local_tensors[qubit_idx]\n",
    "            project_to_state = project_to_states[i]\n",
    "            assert local_tensor.shape[1] == project_to_state.shape[0], (\n",
    "                \"The feature dimension of the project_to_states must match the physical dimension of the local tensor of MPS\"\n",
    "            )\n",
    "            local_tensors[qubit_idx] = einsum(\n",
    "                local_tensor, project_to_state, \"left physical right, physical -> left right\"\n",
    "            )\n",
    "    else:\n",
    "        for i, qubit_idx in enumerate(qubit_indices):\n",
    "            local_tensor = local_tensors[qubit_idx]\n",
    "            state_idx = project_to_states[i]\n",
    "            assert 0 <= state_idx < local_tensor.shape[1], (\n",
    "                f\"state_idx must be in [0, {local_tensor.shape[1]}), got {state_idx}\"\n",
    "            )\n",
    "            local_tensors[qubit_idx] = local_tensor[:, state_idx, :]\n",
    "\n",
    "    # sort qubit_indices in descending order to safely pop tensors\n",
    "    qubit_indices.sort(reverse=True)\n",
    "    RIGHT_DIM = -1\n",
    "    LEFT_DIM = 0\n",
    "    for qubit_idx in qubit_indices[:-1]:\n",
    "        assert qubit_idx > 0\n",
    "        qubit_left = qubit_idx - 1\n",
    "        local_tensors[qubit_left] = torch.tensordot(\n",
    "            local_tensors[qubit_left], local_tensors[qubit_idx], dims=[[RIGHT_DIM], [LEFT_DIM]]\n",
    "        )\n",
    "        local_tensors.pop(qubit_idx)\n",
    "\n",
    "    if len(local_tensors) > 1:\n",
    "        qubit_idx = qubit_indices[-1]\n",
    "        if qubit_idx == 0:\n",
    "            qubit_right = 1\n",
    "            local_tensors[qubit_right] = torch.tensordot(\n",
    "                local_tensors[qubit_idx], local_tensors[qubit_right], dims=[[RIGHT_DIM], [LEFT_DIM]]\n",
    "            )\n",
    "        else:\n",
    "            qubit_left = qubit_idx - 1\n",
    "            local_tensors[qubit_left] = torch.tensordot(\n",
    "                local_tensors[qubit_left], local_tensors[qubit_idx], dims=[[RIGHT_DIM], [LEFT_DIM]]\n",
    "            )\n",
    "\n",
    "        local_tensors.pop(qubit_idx)\n",
    "    else:\n",
    "        # do nothing if there is only one tensor left\n",
    "        pass\n",
    "\n",
    "    for i in range(len(local_tensors)):\n",
    "        local_tensor = local_tensors[i]\n",
    "        if local_tensor.ndimension() == 2:\n",
    "            local_tensor = local_tensor.unsqueeze(1)  # (left, physical, right)\n",
    "        else:\n",
    "            assert local_tensor.ndimension() == 3, \"Unexpected tensor dimension: bug?\"\n",
    "\n",
    "    return local_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export mps.modules\n",
    "from tensor_network.mps.functional import project_multi_qubits\n",
    "\n",
    "# monkey patches from 4-6.ipynb to avoid code clutter in 4-2.ipynb\n",
    "\n",
    "\n",
    "def _project_multi_qubits(\n",
    "    self, qubit_indices: List[int], project_to_states: torch.Tensor | List[int]\n",
    ") -> MPS:\n",
    "    \"\"\"\n",
    "    Do projection of multiple qubits of this MPS, returning a new MPS.\n",
    "\n",
    "    Args:\n",
    "        qubit_indices: List of indices of the qubits to project.\n",
    "        project_to_states: List of states to project to.\n",
    "\n",
    "    Returns:\n",
    "        MPS, the new MPS after projection.\n",
    "    \"\"\"\n",
    "    local_tensors = self._mps\n",
    "    new_local_tensors = project_multi_qubits(local_tensors, qubit_indices, project_to_states)\n",
    "    return MPS(mps_tensors=new_local_tensors)\n",
    "\n",
    "\n",
    "MPS.project_multi_qubits = _project_multi_qubits\n",
    "\n",
    "\n",
    "def _project_one_qubit(self, qubit_idx: int, project_to_state: torch.Tensor | int) -> MPS:\n",
    "    assert isinstance(project_to_state, (torch.Tensor, int)), (\n",
    "        \"project_to_state must be a tensor or an integer\"\n",
    "    )\n",
    "    if isinstance(project_to_state, int):\n",
    "        project_to_states = [project_to_state]\n",
    "    else:\n",
    "        assert project_to_state.ndimension() == 1, \"project_to_state must be a 1D tensor\"\n",
    "        project_to_states = project_to_state.unsqueeze(0)\n",
    "\n",
    "    qubit_indices = [qubit_idx]\n",
    "    return self.project_multi_qubits(qubit_indices, project_to_states)\n",
    "\n",
    "\n",
    "MPS.project_one_qubit = _project_one_qubit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export algorithms.gmps\n",
    "\n",
    "import numpy as np\n",
    "from tensor_network.feature_mapping import cossin_feature_map\n",
    "from copy import deepcopy\n",
    "from typing import Literal, Dict, Any\n",
    "\n",
    "\n",
    "def generate_sample_with_gmps(\n",
    "    mps: MPS,\n",
    "    *,\n",
    "    sample: torch.Tensor | None,\n",
    "    sample_num: int,\n",
    "    gen_indices: List[int] | None,\n",
    "    feature_mapping: Literal[\"cossin\"],\n",
    "    feature_mapping_kwargs: Dict[str, Any] = {},\n",
    "    gen_order: Literal[\"ascending\", \"descending\"] = \"ascending\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a sample with GMPS.\n",
    "\n",
    "    Args:\n",
    "        mps: MPS to generate sample from.\n",
    "        sample: Sample to partially sample from. If None, sample and generate from all qubits.\n",
    "        sample_num: Number of samples to average over.\n",
    "        gen_indices: Indices of the qubits to generate.\n",
    "        feature_mapping: Feature mapping to use.\n",
    "        feature_mapping_kwargs: Keyword arguments for the feature mapping.\n",
    "    \"\"\"\n",
    "    assert sample_num > 0, \"sample_num must be positive\"\n",
    "    assert gen_order in [\"ascending\", \"descending\"], \"gen_order must be 'ascending' or 'descending'\"\n",
    "    assert feature_mapping in [\"cossin\"], \"feature_mapping must be 'cossin'\"\n",
    "    length = mps.length\n",
    "    generate_all = False\n",
    "    if sample is None or gen_indices is None:\n",
    "        if gen_order == \"ascending\":\n",
    "            gen_indices = list(range(length))\n",
    "        else:\n",
    "            gen_indices = list(range(length - 1, -1, -1))\n",
    "        sample = None\n",
    "        generate_all = True\n",
    "\n",
    "    if generate_all:\n",
    "        sample = torch.zeros(length, device=mps.device, dtype=mps.dtype)\n",
    "    else:\n",
    "        assert sample.shape in [(length,), (1, length)], (\n",
    "            f\"sample cannot be batched, got one with shape {sample.shape}\"\n",
    "        )\n",
    "        sample = sample.squeeze()  # (length,)\n",
    "        project_indices = list(range(length))\n",
    "        for gi in gen_indices:\n",
    "            project_indices.remove(gi)\n",
    "        sample_for_projection = sample[project_indices]\n",
    "        if feature_mapping == \"cossin\":\n",
    "            features_for_projection = cossin_feature_map(\n",
    "                sample_for_projection, **feature_mapping_kwargs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Feature mapping {feature_mapping} not supported\")\n",
    "        features_for_projection = features_for_projection.squeeze(\n",
    "            0\n",
    "        )  # get rid of the batch dimension\n",
    "        mps = mps.project_multi_qubits(project_indices, features_for_projection)\n",
    "\n",
    "    # Remap gen_indices in case of a new mps after projection\n",
    "    # the two argsorts effectively give the ranking of each in gen_indices, which is a new item in gen_indices_new\n",
    "    # in the case that sample is None, gen_indices_new is the same as gen_indices\n",
    "    #\n",
    "    # the algorithm is the same as below:\n",
    "    # ```\n",
    "    # indices = np.argsort(np.argsort(gen_indices))\n",
    "    # ranks = [None for _ in len(gen_indices)]\n",
    "    # for i in len(gen_indices):\n",
    "    #     original_index = indices[i]\n",
    "    #     rank = i\n",
    "    #     ranks[original_index] = rank\n",
    "    # gen_indices_new = ranks\n",
    "    # ```\n",
    "    #\n",
    "    # This remapping is needed since project_indices will be projected and removed from the new MPS\n",
    "    gen_indices = np.array(gen_indices)\n",
    "    gen_indices_new = np.argsort(\n",
    "        np.argsort(gen_indices)\n",
    "    )  # TODO: refactor the function - this line is not useful when generate_all is True\n",
    "\n",
    "    # defensive coding\n",
    "    if generate_all:\n",
    "        assert np.all(gen_indices_new == gen_indices), (\n",
    "            \"gen_indices_new should be the same as gen_indices\"\n",
    "        )\n",
    "\n",
    "    samples = []\n",
    "    for _ in range(sample_num):\n",
    "        sample_i = sample.clone()\n",
    "        mps_i = deepcopy(mps)\n",
    "        gen_indices_i = gen_indices_new.copy()\n",
    "        pos = 0\n",
    "        while len(gen_indices_i) > 0:\n",
    "            gen_idx = gen_indices_i[0]\n",
    "            mps_i.center_orthogonalization_(gen_idx, mode=\"qr\", normalize=True)\n",
    "            rdm = mps_i.one_body_reduced_density_matrix(idx=gen_idx, do_tracing=True)\n",
    "            assert rdm.shape == (2, 2)\n",
    "            lm = rdm.diag()\n",
    "            prob_1 = lm[1]\n",
    "            state = torch.bernoulli(prob_1).to(torch.long)\n",
    "            gen_idx_on_sample = gen_indices[pos]\n",
    "            sample_i[gen_idx_on_sample] = state\n",
    "            new_mps_i = mps_i.project_one_qubit(gen_idx, state.cpu().item())\n",
    "            # the center of mps_i is gen_idx and the qubit at gen_idx has been projected\n",
    "            new_mps_i._center = max(0, mps_i.center - 1)\n",
    "            mps_i = new_mps_i\n",
    "            # update gen_indices_i since mps_i has been projected\n",
    "            gen_indices_i[gen_indices_i > gen_idx] -= 1\n",
    "            gen_indices_i = gen_indices_i[1:]\n",
    "            pos += 1\n",
    "\n",
    "        samples.append(sample_i)\n",
    "\n",
    "    samples = torch.stack(samples)\n",
    "    generated_sample = samples.mean(dim=0)\n",
    "    return generated_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters from 4-5.ipynb\n",
    "FEATURE_MAPPING = \"cossin\"\n",
    "FEATURE_MAPPING_KWARGS = {\"theta\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_0_mps = MPS.load_from_safetensors(\"datasets/mps/mnist_0_mps.safetensors\", requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test and debug\n",
    "img_g = generate_sample_with_gmps(\n",
    "    mnist_0_mps,\n",
    "    sample=None,\n",
    "    sample_num=1,\n",
    "    gen_indices=None,\n",
    "    feature_mapping=FEATURE_MAPPING,\n",
    "    feature_mapping_kwargs=FEATURE_MAPPING_KWARGS,\n",
    ")\n",
    "img_g = img_g.reshape(28, 28)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
