"""长度为 5 的开放边界的 MPS"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../4-1.ipynb.

# %% auto 0
__all__ = ['MPSType', 'gen_random_mps_tensors', 'calc_global_tensor_by_contract', 'calc_global_tensor_by_tensordot',
           'calculate_mps_norm_factors', 'normalize_mps']

# %% ../../4-1.ipynb 2
import torch
from ..tensor_utils import tensor_contract
from typing import List
from enum import Enum

# %% ../../4-1.ipynb 3
class MPSType(Enum):
    """
    The type of the MPS
    """

    Open = "Open"
    Periodic = "Periodic"

    @staticmethod
    def get_mps_type(mps_tensors: List[torch.Tensor]) -> "MPSType":
        """
        Determine the type of the MPS
        """
        if mps_tensors[0].shape[0] == 1 and mps_tensors[-1].shape[2] == 1:
            return MPSType.Open
        else:
            return MPSType.Periodic


def gen_random_mps_tensors(
    length: int,
    physical_dim: int,
    virtual_dim: int,
    mps_type: MPSType,
    dtype: torch.dtype | None = None,
    device: torch.device | None = None,
) -> List[torch.Tensor]:
    """
    Generate random MPS tensors

    Args:
        length: int, the length of the MPS
        physical_dim: int, the physical dimension of the MPS
        virtual_dim: int, the virtual dimension of the MPS
        mps_type: MPSType, the type of the MPS
        dtype: torch.dtype, the dtype of the MPS
        device: torch.device, the device of the MPS

    Returns:
        List[torch.Tensor], the MPS tensors
    """
    if mps_type == MPSType.Open:
        mps_tensors = []
        mps_tensors.append(torch.randn(1, physical_dim, virtual_dim, dtype=dtype, device=device))
        rand_tensor = torch.randn(
            length - 2, virtual_dim, physical_dim, virtual_dim, dtype=dtype, device=device
        )
        for i in range(length - 2):
            mps_tensors.append(rand_tensor[i])
        mps_tensors.append(torch.randn(virtual_dim, physical_dim, 1, dtype=dtype, device=device))
        return mps_tensors
    elif mps_type == MPSType.Periodic:
        mps_tensor = torch.randn(
            length, virtual_dim, physical_dim, virtual_dim, dtype=dtype, device=device
        )
        return [mps_tensor[i] for i in range(length)]
    else:
        raise NotImplementedError(f"MPS type {mps_type} is not implemented")


def calc_global_tensor_by_contract(mps_tensors: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculate the global tensor by contracting the MPS tensors

    Args:
        mps_tensors: List[torch.Tensor], the MPS tensors

    Returns:
        torch.Tensor, the global tensor
    """

    length = len(mps_tensors)
    dim_names = []
    for i in range(length):
        dim_name_list = [f"t{i}{j}" for j in range(3)]
        dim_names.append(dim_name_list)

    input_expression = ",".join([" ".join(dim_names[i]) for i in range(length)])
    output_dims = [dim_name_list[1] for dim_name_list in dim_names]
    output_dims = [dim_names[0][0]] + output_dims + [dim_names[-1][-1]]
    output_expression = " ".join(output_dims)

    contract_dims = []
    for i in range(length - 1):
        right_dim = dim_names[i][2]
        left_dim = dim_names[i + 1][0]
        contract_dims.append({right_dim, left_dim})

    mps_type = MPSType.get_mps_type(mps_tensors)

    if mps_type == MPSType.Periodic:
        front_left_dim = dim_names[0][0]
        end_right_dim = dim_names[-1][2]
        contract_dims.append({front_left_dim, end_right_dim})

    return tensor_contract(
        *mps_tensors, ein_expr=f"{input_expression} -> {output_expression}", dims=contract_dims
    ).squeeze()


def calc_global_tensor_by_tensordot(mps_tensors: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculate the global tensor by tensordot

    Args:
        mps_tensors: List[torch.Tensor], the MPS tensors

    Returns:
        torch.Tensor, the global tensor
    """
    length = len(mps_tensors)
    psi = mps_tensors[0]
    for i in range(1, length):
        psi = torch.tensordot(psi, mps_tensors[i], dims=([-1], [0]))

    mps_type = MPSType.get_mps_type(mps_tensors)

    if mps_type == MPSType.Periodic:
        return torch.einsum("a...a->...", psi)
    elif mps_type == MPSType.Open:
        return psi.squeeze()
    else:
        raise NotImplementedError(f"MPS type {mps_type} is not implemented")

# %% ../../4-1.ipynb 10
def calculate_mps_norm_factors(
    mps_tensors: List[torch.Tensor], __efficient_mode: bool = True
) -> torch.Tensor:
    """
    Calculate the norm factors of the MPS

    Args:
        mps_tensors: List[torch.Tensor], the MPS tensors
        __efficient_mode: bool, whether to use efficient mode

    Returns:
        torch.Tensor, the norm factors
    """

    assert len(mps_tensors) >= 1, "MPS must have at least one tensor"
    conjugates = [t.conj() for t in mps_tensors]
    length = len(conjugates)
    device = conjugates[0].device
    dtype = conjugates[0].dtype
    v = torch.ones(1, 1, dtype=dtype, device=device)  # dims: a b
    norm_factors = torch.empty(length, dtype=dtype, device=device)
    if __efficient_mode:
        for i in range(length):
            v = torch.einsum("ab,aix->bix", v, conjugates[i])
            v = torch.einsum("bix,biy->xy", v, mps_tensors[i])
            norm_factor = v.norm()
            v /= norm_factor
            norm_factors[i] = norm_factor
    else:
        for i in range(length):
            v = torch.einsum("ab,aix,biy->xy", v, conjugates[i], mps_tensors[i])
            norm_factor = v.norm()
            v /= norm_factor
            norm_factors[i] = norm_factor

    return norm_factors

# %% ../../4-1.ipynb 15
def normalize_mps(mps_tensors: List[torch.Tensor]) -> List[torch.Tensor]:
    """
    Normalize the MPS

    Args:
        mps_tensors: List[torch.Tensor], the MPS tensors

    Returns:
        List[torch.Tensor], the normalized MPS tensors
    """
    norms = calculate_mps_norm_factors(mps_tensors)
    normalization_factors = 1 / norms.sqrt()
    middle_tensors = torch.stack(
        mps_tensors[1:-1]
    )  # (length - 2, virtual_dim, physical_dim, virtual_dim)
    middle_factors = normalization_factors[1:-1].reshape(-1, 1, 1, 1)
    normalized_middle_tensors = middle_tensors * middle_factors
    front_tensor = mps_tensors[0] * normalization_factors[0]
    end_tensor = mps_tensors[-1] * normalization_factors[-1]
    return [front_tensor] + [normalized_middle_tensors[i] for i in range(length - 2)] + [end_tensor]
