"""核心是波恩概率诠释，见 [2.1](2-1.ipynb)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../4-5.ipynb.

# %% auto 0
__all__ = ['train_gmps']

# %% ../../4-5.ipynb 2
import torch
from ..mps.modules import MPS, MPSType
from einops import einsum
from tqdm.auto import tqdm
from typing import Tuple

# %% ../../4-5.ipynb 3
def train_gmps(
    *,
    samples: torch.Tensor,
    batch_size: int,
    mps: MPS,
    sweep_times: int,
    lr: float,
    device: torch.device,
    enable_tsgo: bool,
) -> Tuple[torch.Tensor, MPS]:
    assert samples.ndim == 3  # (batch, feature_num, feature_dim)

    def samples_at(idx):
        return samples[:, idx, :]  # (batch, feature_dim)

    EPS = 1e-10
    batch_size, feature_num, feature_dim = samples.shape
    assert feature_num == mps.length
    # set default device to device
    prev_device = torch.get_default_device()
    torch.set_default_device(device)
    # prepare mps, normalize first to avoid numerical instability
    mps.normalize_()
    mps.center_orthogonalization_(0, mode="qr")
    mps_local_tensors = mps._mps  # CAREFUL for inplace operation
    # prepare aux variables
    norm_factors = torch.ones(batch_size, feature_num)
    env_vectors_left = [None] * mps.length
    left_virtual_dim = mps_local_tensors[0].shape[0]
    env_vectors_left[0] = torch.ones(batch_size, left_virtual_dim)
    env_vectors_right = [None] * mps.length
    right_virtual_dim = mps_local_tensors[-1].shape[-1]
    env_vectors_right[-1] = torch.ones(batch_size, right_virtual_dim)

    # prepare env vectors from left to right
    def calc_left_to_right_step(current_tensor, current_env_vector_left, current_sample):
        next_env_vector_left = einsum(
            current_env_vector_left,
            current_sample,
            current_tensor,
            "batch left, batch physical, left physical right -> batch right",
        )
        current_norm_factor = next_env_vector_left.norm(dim=1, keepdim=True)
        return next_env_vector_left / (current_norm_factor + EPS), current_norm_factor.squeeze(-1)

    for idx in range(mps.center):
        next_env_vector_left, current_norm_factor = calc_left_to_right_step(
            mps_local_tensors[idx],
            env_vectors_left[idx],
            samples_at(idx),
        )
        # set the norm factor for current position to be the norm of the next env vector, since the next env vector is to be normalized
        norm_factors[:, idx] = current_norm_factor
        env_vectors_left[idx + 1] = next_env_vector_left

    # prepare env vectors from right to left
    def calc_right_to_left_step(current_tensor, current_env_vector_right, current_sample):
        next_env_vector_right = einsum(
            current_env_vector_right,
            current_sample,
            current_tensor,
            "batch right, batch physical, left physical right -> batch left",
        )
        current_norm_factor = next_env_vector_right.norm(dim=1, keepdim=True)
        return next_env_vector_right / (current_norm_factor + EPS), current_norm_factor.squeeze(-1)

    for idx in range(mps.length - 1, mps.center, -1):
        next_env_vector_right, current_norm_factor = calc_right_to_left_step(
            mps_local_tensors[idx],
            env_vectors_right[idx],
            samples_at(idx),
        )
        assert not current_norm_factor.min().isnan()
        norm_factors[:, idx] = current_norm_factor
        env_vectors_right[idx - 1] = next_env_vector_right

    # update the norm factor at the center
    norm_factors[:, mps.center] = einsum(
        mps_local_tensors[mps.center],
        env_vectors_left[mps.center],
        samples[:, mps.center, :],
        env_vectors_right[mps.center],
        "left physical right, batch left, batch physical, batch right -> batch",
    )

    def calc_nll(norm_factors):
        nll = -2 * torch.log(norm_factors.abs() + EPS).sum(dim=1)  # (batch)
        nll_avg = nll.mean()
        return nll_avg

    init_nll = calc_nll(norm_factors)

    nll_losses = [init_nll]
    print(f"Initially, nll = {init_nll}")

    @torch.compile
    def calc_gradient(
        env_left_vector, env_right_vector, current_sample, current_tensor, enable_tsgo
    ):
        raw_grad = einsum(
            env_left_vector,
            current_sample,
            env_right_vector,
            "batch left, batch physical, batch right -> batch right physical left",
        )
        norm = einsum(
            current_tensor,
            raw_grad,
            "left physical right, batch right physical left -> batch",
        )
        norm += torch.sign(norm) * EPS  # add a small number to avoid division by zero
        grad = (raw_grad / norm.view(-1, 1, 1, 1)).mean(dim=0)
        grad = 2 * (current_tensor - grad)
        grad_shape = grad.shape
        assert grad_shape == current_tensor.shape
        if enable_tsgo:
            grad = grad.flatten()
            current_tensor = current_tensor.flatten()
            projection = torch.dot(grad, current_tensor) * current_tensor
            grad = grad - projection
            grad = grad.reshape(grad_shape)

        grad /= grad.norm()
        return grad

    progress_bar = tqdm(range(sweep_times))
    for i in progress_bar:
        for idx in range(mps.length):
            assert idx == mps.center
            grad = calc_gradient(
                env_vectors_left[idx],
                env_vectors_right[idx],
                samples_at(idx),
                mps_local_tensors[idx],
                enable_tsgo,
            )
            # update the tensor with gradient, inplace operation, will change mps._mps
            mps_local_tensors[idx] -= lr * grad

            # prepare for the next iteration
            if idx < mps.length - 1:
                # move the center to the right
                # the local tensors (mps._mps) at idx and idx + 1 will be changed
                mps.center_orthogonalization_(idx + 1, mode="qr")
                # so we need to update aux variables, only env_vectors_left affected
                new_next_env_vector_left, new_norm_factor = calc_left_to_right_step(
                    mps_local_tensors[idx],
                    env_vectors_left[idx],
                    samples_at(idx),
                )
                env_vectors_left[idx + 1] = new_next_env_vector_left
                norm_factors[:, idx] = new_norm_factor
            else:
                # same as normalize the center tensor, as the center tensor is already at this place
                # we need normalization here to make mps as a unit norm state so to preserve the probability interpretation
                mps.normalize_()

        for idx in range(mps.length - 1, -1, -1):
            assert idx == mps.center
            grad = calc_gradient(
                env_vectors_left[idx],
                env_vectors_right[idx],
                samples_at(idx),
                mps_local_tensors[idx],
                enable_tsgo,
            )
            mps_local_tensors[idx] -= lr * grad

            # prepare for the next iteration
            if idx > 0:
                # move the center to the left
                # the local tensors (mps._mps) at idx and idx - 1 will be changed
                mps.center_orthogonalization_(idx - 1, mode="qr")
                # so we need to update aux variables, only env_vectors_right affected
                new_next_env_vector_right, new_norm_factor = calc_right_to_left_step(
                    mps_local_tensors[idx],
                    env_vectors_right[idx],
                    samples_at(idx),
                )
                env_vectors_right[idx - 1] = new_next_env_vector_right
                norm_factors[:, idx] = new_norm_factor
            else:
                # same as normalize the center tensor, as the center tensor is already at this place
                # we need normalization here to make mps as a unit norm state so to preserve the probability interpretation
                mps.normalize_()

        assert mps.center == 0
        # update the norm factor at the center
        norm_factors[:, mps.center] = einsum(
            mps_local_tensors[mps.center],
            env_vectors_left[mps.center],
            samples[:, mps.center, :],
            env_vectors_right[mps.center],
            "left physical right, batch left, batch physical, batch right -> batch",
        )
        nll_loss = calc_nll(norm_factors)
        nll_losses.append(nll_loss)
        progress_bar.set_description(f"Iter {i} NLL: {nll_loss:.4f}")

    # restore the default device
    torch.set_default_device(prev_device)
    return torch.stack(nll_losses), mps
