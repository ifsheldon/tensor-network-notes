"""核心是波恩概率诠释，见 [2.1](2-1.ipynb)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../4-5.ipynb.

# %% auto 0
__all__ = ['EPS', 'calc_left_to_right_step', 'calc_right_to_left_step', 'calc_nll', 'calc_gradient', 'eval_nll', 'train_gmps']

# %% ../../4-5.ipynb 2
import torch
from ..mps.modules import MPS, MPSType
from einops import einsum
from tqdm.auto import tqdm
from typing import Tuple, List
from torch.utils.data import DataLoader, TensorDataset

# %% ../../4-5.ipynb 3
EPS = 1e-10


# prepare env vectors from left to right
def calc_left_to_right_step(
    current_tensor: torch.Tensor,
    current_env_vector_left: torch.Tensor,
    current_sample: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Calculate one step from left to right of the sweep algorithm
    """
    next_env_vector_left = einsum(
        current_env_vector_left,
        current_sample,
        current_tensor,
        "batch left, batch physical, left physical right -> batch right",
    )
    current_norm_factor = next_env_vector_left.norm(dim=1, keepdim=True)
    return next_env_vector_left / (current_norm_factor + EPS), current_norm_factor.squeeze(-1)


def calc_right_to_left_step(
    current_tensor: torch.Tensor,
    current_env_vector_right: torch.Tensor,
    current_sample: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Calculate one step from right to left of the sweep algorithm
    """
    next_env_vector_right = einsum(
        current_env_vector_right,
        current_sample,
        current_tensor,
        "batch right, batch physical, left physical right -> batch left",
    )
    current_norm_factor = next_env_vector_right.norm(dim=1, keepdim=True)
    return next_env_vector_right / (current_norm_factor + EPS), current_norm_factor.squeeze(-1)


def calc_nll(norm_factors: torch.Tensor) -> torch.Tensor:
    """
    Calculate the negative log likelihood from the norm factors in a batch
    """
    nll = -2 * torch.log(norm_factors.abs() + EPS).sum(dim=1)  # (batch)
    return nll


@torch.compile
def calc_gradient(
    env_left_vector: torch.Tensor,
    env_right_vector: torch.Tensor,
    current_sample: torch.Tensor,
    current_tensor: torch.Tensor,
    enable_tsgo: bool,
) -> torch.Tensor:
    """
    Calculate the gradient w.r.t. the current tensor
    """
    raw_grad = einsum(
        env_left_vector,
        current_sample,
        env_right_vector,
        "batch left, batch physical, batch right -> batch left physical right",
    )
    norm = einsum(
        current_tensor,
        raw_grad,
        "left physical right, batch left physical right -> batch",
    )
    norm += torch.sign(norm) * EPS  # add a small number to avoid division by zero
    grad = (raw_grad / norm.view(-1, 1, 1, 1)).mean(dim=0)
    grad = 2 * (current_tensor - grad)
    grad_shape = grad.shape
    assert grad_shape == current_tensor.shape
    if enable_tsgo:
        grad = grad.flatten()
        current_tensor = current_tensor.flatten()
        projection = torch.dot(grad, current_tensor) * current_tensor
        grad = grad - projection
        grad = grad.reshape(grad_shape)

    grad /= grad.norm()
    return grad


def eval_nll(
    *,
    samples: torch.Tensor,
    mps: MPS,
    device: torch.device,
) -> torch.Tensor:
    assert samples.ndim == 3  # (dataset_size, feature_num, feature_dim)
    assert mps.center is not None
    dataset_size, feature_num, _ = samples.shape
    assert feature_num == mps.length
    # set default device to device
    prev_device = torch.get_default_device()
    torch.set_default_device(device)
    mps_local_tensors = mps.local_tensors
    batch_size = dataset_size  # since we do the init NLL evaluation in one go
    env_vectors_left: List[torch.Tensor | None] = [None] * mps.length
    left_virtual_dim = mps_local_tensors[0].shape[0]
    env_vectors_left[0] = torch.ones(batch_size, left_virtual_dim)
    env_vectors_right: List[torch.Tensor | None] = [None] * mps.length
    right_virtual_dim = mps_local_tensors[-1].shape[-1]
    env_vectors_right[-1] = torch.ones(batch_size, right_virtual_dim)
    norm_factors = torch.ones(batch_size, feature_num)

    def samples_at(idx):
        return samples[:, idx, :]  # (batch, feature_dim)

    for idx in range(mps.center):
        next_env_vector_left, current_norm_factor = calc_left_to_right_step(
            mps_local_tensors[idx],
            env_vectors_left[idx],
            samples_at(idx),
        )
        # set the norm factor for current position to be the norm of the next env vector, since the next env vector is to be normalized
        norm_factors[:, idx] = current_norm_factor
        env_vectors_left[idx + 1] = next_env_vector_left

    # prepare env vectors from right to left
    for idx in range(mps.length - 1, mps.center, -1):
        next_env_vector_right, current_norm_factor = calc_right_to_left_step(
            mps_local_tensors[idx],
            env_vectors_right[idx],
            samples_at(idx),
        )
        norm_factors[:, idx] = current_norm_factor
        env_vectors_right[idx - 1] = next_env_vector_right

    # update the norm factor at the center
    norm_factors[:, mps.center] = einsum(
        mps_local_tensors[mps.center],
        env_vectors_left[mps.center],
        samples_at(mps.center),
        env_vectors_right[mps.center],
        "left physical right, batch left, batch physical, batch right -> batch",
    )

    nll = calc_nll(norm_factors).mean()
    # restore the default device
    torch.set_default_device(prev_device)
    return nll


def train_gmps(
    *,
    samples: torch.Tensor,
    batch_size: int,
    mps: MPS,
    sweep_times: int,
    lr: float,
    device: torch.device,
    enable_tsgo: bool,
) -> Tuple[torch.Tensor, MPS]:
    dataset_size = samples.shape[0]
    assert dataset_size % batch_size == 0
    # prepare mps, normalize first to avoid numerical instability
    # mps.normalize_()
    mps.center_orthogonalization_(0, mode="qr", normalize=True, check_nan=True)
    init_nll = eval_nll(samples=samples, mps=mps, device=device)
    print(f"Initially, nll = {init_nll}")

    # set default device to device
    prev_device = torch.get_default_device()
    torch.set_default_device(device)
    # prepare dataloader
    dataloader = DataLoader(TensorDataset(samples), batch_size=batch_size, shuffle=True)

    mps_local_tensors = mps._mps  # CAREFUL for inplace operation
    feature_num = mps.length
    left_virtual_dim = mps_local_tensors[0].shape[0]
    right_virtual_dim = mps_local_tensors[-1].shape[-1]

    nll_losses = [init_nll]

    progress_bar = tqdm(range(sweep_times))
    for i in progress_bar:
        epoch_nll_losses = []
        for batch_data in tqdm(dataloader, leave=False):
            batch_data = batch_data[0]
            batch_size = batch_data.shape[0]
            # prepare aux variables
            env_vectors_left: List[torch.Tensor | None] = [None] * mps.length
            env_vectors_left[0] = torch.ones(batch_size, left_virtual_dim)
            env_vectors_right: List[torch.Tensor | None] = [None] * mps.length
            env_vectors_right[-1] = torch.ones(batch_size, right_virtual_dim)
            norm_factors = torch.ones(batch_size, feature_num)

            def data_at(idx):
                return batch_data[:, idx, :]  # (batch, feature_dim)

            # prepare env vectors from right to left
            # leave out left-to-right because the center of mps always starts at 0
            for idx in range(mps.length - 1, mps.center, -1):
                next_env_vector_right, current_norm_factor = calc_right_to_left_step(
                    mps_local_tensors[idx],
                    env_vectors_right[idx],
                    data_at(idx),
                )
                norm_factors[:, idx] = current_norm_factor
                env_vectors_right[idx - 1] = next_env_vector_right

            # update the norm factor at the center
            norm_factors[:, mps.center] = einsum(
                mps_local_tensors[mps.center],
                env_vectors_left[mps.center],
                data_at(mps.center),
                env_vectors_right[mps.center],
                "left physical right, batch left, batch physical, batch right -> batch",
            )

            # gradient calculation and optimization, from left to right
            for idx in range(mps.length):
                assert idx == mps.center
                grad = calc_gradient(
                    env_vectors_left[idx],
                    env_vectors_right[idx],
                    data_at(idx),
                    mps_local_tensors[idx],
                    enable_tsgo,
                )
                # update the tensor with gradient, inplace operation, will change mps._mps
                mps_local_tensors[idx] -= lr * grad

                # prepare for the next iteration
                if idx < mps.length - 1:
                    # move the center to the right
                    # the local tensors (mps._mps) at idx and idx + 1 will be changed
                    mps.center_orthogonalization_(idx + 1, mode="qr", normalize=True)
                    # so we need to update aux variables, only env_vectors_left affected
                    new_next_env_vector_left, new_norm_factor = calc_left_to_right_step(
                        mps_local_tensors[idx],
                        env_vectors_left[idx],
                        data_at(idx),
                    )
                    env_vectors_left[idx + 1] = new_next_env_vector_left
                    norm_factors[:, idx] = new_norm_factor
                else:
                    # same as normalize the center tensor, as the center tensor is already at this place
                    # we need normalization here to make mps as a unit norm state so to preserve the probability interpretation
                    mps.normalize_()

            for idx in range(mps.length - 1, -1, -1):
                assert idx == mps.center
                grad = calc_gradient(
                    env_vectors_left[idx],
                    env_vectors_right[idx],
                    data_at(idx),
                    mps_local_tensors[idx],
                    enable_tsgo,
                )
                mps_local_tensors[idx] -= lr * grad
                # prepare for the next iteration
                if idx > 0:
                    # move the center to the left
                    # the local tensors (mps._mps) at idx and idx - 1 will be changed
                    mps.center_orthogonalization_(idx - 1, mode="qr", normalize=True)
                    # so we need to update aux variables, only env_vectors_right affected
                    new_next_env_vector_right, new_norm_factor = calc_right_to_left_step(
                        mps_local_tensors[idx],
                        env_vectors_right[idx],
                        data_at(idx),
                    )
                    env_vectors_right[idx - 1] = new_next_env_vector_right
                    norm_factors[:, idx] = new_norm_factor
                else:
                    # same as normalize the center tensor, as the center tensor is already at this place
                    # we need normalization here to make mps as a unit norm state so to preserve the probability interpretation
                    mps.normalize_()

            assert mps.center == 0
            # update the norm factor at the center
            norm_factors[:, mps.center] = einsum(
                mps_local_tensors[mps.center],
                env_vectors_left[mps.center],
                data_at(mps.center),
                env_vectors_right[mps.center],
                "left physical right, batch left, batch physical, batch right -> batch",
            )
            batch_nll_loss = calc_nll(norm_factors)
            epoch_nll_losses.append(batch_nll_loss)

        epoch_nll_loss = torch.cat(epoch_nll_losses).mean()
        nll_losses.append(epoch_nll_loss)
        progress_bar.set_description(f"Iter {i} NLL: {epoch_nll_loss:.4f}")

    # restore the default device
    torch.set_default_device(prev_device)
    return torch.stack(nll_losses), mps
